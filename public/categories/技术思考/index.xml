<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>技术思考 on kingdeguo&#39;s blog</title>
    <link>https://www.kingdeguo.com/categories/%E6%8A%80%E6%9C%AF%E6%80%9D%E8%80%83/</link>
    <description>Recent content in 技术思考 on kingdeguo&#39;s blog</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 10 Jan 2026 11:15:00 +0800</lastBuildDate>
    <atom:link href="https://www.kingdeguo.com/categories/%E6%8A%80%E6%9C%AF%E6%80%9D%E8%80%83/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>组织内部价值坐标系的重排 | 读《 Anthropic Economic Index》报告</title>
      <link>https://www.kingdeguo.com/2026/01/10/%E7%BB%84%E7%BB%87%E5%86%85%E9%83%A8%E4%BB%B7%E5%80%BC%E5%9D%90%E6%A0%87%E7%B3%BB%E7%9A%84%E9%87%8D%E6%8E%92-%E8%AF%BB-anthropic-economic-index%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Sat, 10 Jan 2026 11:15:00 +0800</pubDate>
      <guid>https://www.kingdeguo.com/2026/01/10/%E7%BB%84%E7%BB%87%E5%86%85%E9%83%A8%E4%BB%B7%E5%80%BC%E5%9D%90%E6%A0%87%E7%B3%BB%E7%9A%84%E9%87%8D%E6%8E%92-%E8%AF%BB-anthropic-economic-index%E6%8A%A5%E5%91%8A/</guid>
      <description>&lt;h1 id=&#34;组织内部价值坐标系的重排--读-anthropic-economic-index报告&#34;&gt;组织内部价值坐标系的重排 | 读《 Anthropic Economic Index》报告&lt;/h1&gt;&#xA;&lt;p&gt;我最近反复读了一份报告：Anthropic 在 2025 年发布并持续更新的 Anthropic Economic Index。&lt;/p&gt;&#xA;&lt;p&gt;它吸引我的地方不在于&amp;quot;结论多新&amp;quot;，而在于它换了一种看问题的方式。&lt;/p&gt;&#xA;&lt;p&gt;大多数关于 AI 的讨论，习惯直接问一个问题：这个岗位会不会消失？&lt;/p&gt;&#xA;&lt;p&gt;而这份报告刻意回避了这个提法，它做了一件更&amp;quot;笨&amp;quot;、但也更接近现实的事——把岗位拆成任务。&lt;/p&gt;&#xA;&lt;p&gt;在它的分析框架里，没有&amp;quot;程序员&amp;quot;、&amp;ldquo;分析师&amp;rdquo;、&amp;ldquo;编辑&amp;quot;这些整体概念，只有具体的工作单元：写一段代码、整理一份数据、生成一版初稿、校对一次逻辑。AI 的渗透，是发生在这些任务层面的，而不是一刀切地作用在岗位上。&lt;/p&gt;&#xA;&lt;p&gt;这个视角非常关键。因为一旦你接受&amp;quot;岗位不会整体消失，但内部结构会被重排&amp;rdquo;，后面的很多现象就变得可解释了。&lt;/p&gt;&#xA;&lt;p&gt;从真实使用数据看，AI 覆盖得最快的，是文本处理、逻辑推演、规则明确的任务；几乎不涉及需要物理操作、复杂情绪判断或高度情境化协作的部分。&lt;/p&gt;&#xA;&lt;p&gt;这意味着，大多数岗位并没有被替代，而是被重新分工：基础、标准化的任务被迅速压缩，剩下的工作则越来越集中在判断、整合和责任承担上。&lt;/p&gt;&#xA;&lt;p&gt;这也是为什么，现实中并不存在&amp;quot;额外付出完全没有回报&amp;quot;这件事。回报并没有消失，而是不再平均地分布在&amp;quot;多干一点&amp;quot;上。&lt;/p&gt;&#xA;&lt;p&gt;当基础任务被 AI 吃掉之后，组织真正开始付费的，是另一类东西：在不确定条件下做决定、在系统缺口中兜底、在风险发生前承担责任。这些东西很重要，但有一个问题——它们很难被标准化评估。&lt;/p&gt;&#xA;&lt;p&gt;于是，一个熟悉的张力出现了：一方面，组织仍然在奖励高绩效；另一方面，很多人却感觉这种奖励越来越&amp;quot;不透明&amp;quot;。&lt;/p&gt;&#xA;&lt;p&gt;这并不完全是管理失灵，而是价值识别机制滞后于价值结构变化。&lt;/p&gt;&#xA;&lt;p&gt;当贡献无法完全通过流程、指标、考核被识别时，它就会通过信任、授权、默契这些非正式方式被识别。于是，我们看到一些被简单归因为&amp;quot;关系&amp;quot;、&amp;ldquo;嫡系&amp;rdquo;、&amp;ldquo;站队&amp;quot;的现象。但从结构上看，这更像是：判断权已经开始值钱，但还没有被正式定价。&lt;/p&gt;&#xA;&lt;p&gt;Anthropic 的后续数据其实进一步印证了这一点。企业场景中，AI 的使用更偏向自动化，且集中在流程稳定、目标明确的环节；而真正高价值、影响方向的决策任务，AI 仍然只能辅助。这导致一个结果：系统性工作的&amp;quot;人&amp;quot;在减少，系统外判断的&amp;quot;人&amp;quot;在变贵。&lt;/p&gt;&#xA;&lt;p&gt;从决策者的角度看，这里真正棘手的，不是&amp;quot;要不要用 AI&amp;rdquo;，而是两个更现实的问题：&lt;/p&gt;&#xA;&lt;p&gt;第一，组织是否愿意为判断型价值付出结构成本，把原本依赖个人关系和隐性信任的贡献，逐步拉回到制度中？&lt;/p&gt;&#xA;&lt;p&gt;第二，如果不愿意，那么是否接受一个事实：非正式结构会继续存在，并且会越来越影响资源分配。&lt;/p&gt;&#xA;&lt;p&gt;对个体也是同样的逻辑。真正被压缩的，不是能力本身，而是可替代的那一部分能力。而能持续产生议价空间的，是那些仍然处在&amp;quot;任务拆解之外&amp;quot;的判断和责任。&lt;/p&gt;&#xA;&lt;p&gt;所以，AI 对劳动力市场的影响，本质上不是岗位数量的变化，而是组织内部价值坐标系的重排。&lt;/p&gt;&#xA;&lt;p&gt;谁能被系统吸收，谁只能靠关系被识别，这条分界线，正在变得越来越清晰。&lt;/p&gt;&#xA;&lt;p&gt;这也是为什么，这一轮变化更像一次压力测试。它测试的不是&amp;quot;谁会被淘汰&amp;quot;，而是谁的价值，已经不再适配原有的组织定价方式。&lt;/p&gt;</description>
    </item>
    <item>
      <title>2023—2025，从大模型看组织进化论</title>
      <link>https://www.kingdeguo.com/2026/01/01/20232025%E4%BB%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9C%8B%E7%BB%84%E7%BB%87%E8%BF%9B%E5%8C%96%E8%AE%BA/</link>
      <pubDate>Thu, 01 Jan 2026 11:59:00 +0800</pubDate>
      <guid>https://www.kingdeguo.com/2026/01/01/20232025%E4%BB%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9C%8B%E7%BB%84%E7%BB%87%E8%BF%9B%E5%8C%96%E8%AE%BA/</guid>
      <description>&lt;h1 id=&#34;20232025从大模型看组织进化论&#34;&gt;2023—2025，从大模型看组织进化论&lt;/h1&gt;&#xA;&lt;p&gt;一开始，我其实并不觉得这是&amp;quot;组织层面&amp;quot;的问题。&lt;/p&gt;&#xA;&lt;p&gt;2023 年，大模型刚进公司时，我们讨论的还只是效率。谁用得快，谁写得准，谁能多省一点人力成本。那时候的共识很朴素：这是一个工具问题，而不是结构问题。工具好不好，用不用，更多取决于个人能力与态度。组织只需要&amp;quot;跟上&amp;quot;，不需要&amp;quot;重构&amp;quot;。&lt;/p&gt;&#xA;&lt;p&gt;后来我发现，这个判断太天真了。&lt;/p&gt;&#xA;&lt;p&gt;当少数人开始借助模型，把原本需要几天的工作压缩到几个小时，组织里第一次出现了一种微妙的不平衡。不是能力差距，而是节奏差距。原有的流程、评审、决策节拍，开始拖住那些已经&amp;quot;跑快了&amp;quot;的人。那一刻我才意识到，问题不在工具，而在结构——结构决定了速度的上限。&lt;/p&gt;&#xA;&lt;p&gt;于是我们开始&amp;quot;加系统&amp;quot;。&lt;/p&gt;&#xA;&lt;p&gt;2024 年，组织层面开始更系统地引入大模型，搭平台、建中台、定规范。看起来一切都在进步，但新的问题很快浮现：系统越复杂，组织反而越迟钝。中层变得更忙，却不是在做判断，而是在解释系统、协调流程、修补例外。技术在前进，组织却在变重。&lt;/p&gt;&#xA;&lt;p&gt;那时候我第一次产生强烈的不适感：我们是不是在用&amp;quot;智能&amp;quot;，加固一个本该被打破的结构？&lt;/p&gt;&#xA;&lt;p&gt;真正的转折发生在一次失败之后。&lt;/p&gt;&#xA;&lt;p&gt;一个被寄予厚望的 AI 项目，没有死在技术上，而是死在协作上。模型没问题，数据没问题，甚至产出也没问题，但就是落不了地。复盘时，所有人都&amp;quot;按流程做对了事&amp;quot;，却没有一个人对结果负责。那一刻我突然意识到一个残酷的事实：当智能进入组织，传统的问责与协作机制，反而成了风险源。&lt;/p&gt;&#xA;&lt;p&gt;从那之后，我才开始重新理解&amp;quot;组织进化&amp;quot;。&lt;/p&gt;&#xA;&lt;p&gt;组织并不是慢半拍，而是它的进化逻辑，和技术的进化逻辑不一样。技术追求的是能力跃迁，而组织追求的是稳定可控。当大模型开始具备&amp;quot;类似判断&amp;quot;的能力时，原本用来保证稳定的科层制、流程制、审批制，反而开始系统性地压制效率与创造力。&lt;/p&gt;&#xA;&lt;p&gt;于是，一种新的组织形态开始显影。&lt;/p&gt;&#xA;&lt;p&gt;它不再强调&amp;quot;谁汇报给谁&amp;quot;，而是强调能力如何被快速调用；不再要求中层&amp;quot;盯人&amp;quot;，而是要求他们设计工作流与验证机制；不再把知识当作文档沉淀，而是把知识当作可以被模型持续学习、反向喂养业务的循环系统。&lt;/p&gt;&#xA;&lt;p&gt;这不是一次改革，而是一种被现实逼出来的调整。&lt;/p&gt;&#xA;&lt;p&gt;到 2025 年，我终于明白了一件事：大模型并没有改变组织的本质，但它极端放大了组织原本的问题。好的结构，会被智能放大成杠杆；坏的结构，会被智能放大成灾难。&lt;/p&gt;&#xA;&lt;p&gt;也是在这一刻，我对&amp;quot;组织进化论&amp;quot;有了真正的敬畏。&lt;/p&gt;&#xA;&lt;p&gt;进化从来不是选择题。当环境改变，组织要么重构自己，要么被重构。区别只在于，是主动，还是被动。&lt;/p&gt;&#xA;&lt;p&gt;回头再看 2023 年那些&amp;quot;只是工具&amp;quot;的判断，我并不觉得可笑。那是每一个组织在面对范式变化时，都会经历的第一反应。只是，有些组织停在了那里，而有些，被现实推着，继续往前走了几步。&lt;/p&gt;&#xA;&lt;p&gt;而这几步，往往决定了未来三到五年的位置。&lt;/p&gt;&#xA;&lt;p&gt;这可能不是一篇给出答案的文章，但它至少来自一个走错过、慢过、被现实教育过的视角。&lt;/p&gt;&#xA;&lt;p&gt;在智能时代，真正决定组织命运的，从来不是模型本身，而是——你是否愿意为认知升级，付出结构重构的代价。&lt;/p&gt;&#xA;&lt;p&gt;这一点，我是吃过亏，才学会的。&lt;/p&gt;</description>
    </item>
    <item>
      <title>思维实验：从尼采看大语言模型边界</title>
      <link>https://www.kingdeguo.com/2025/11/16/%E6%80%9D%E7%BB%B4%E5%AE%9E%E9%AA%8C%E4%BB%8E%E5%B0%BC%E9%87%87%E7%9C%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BE%B9%E7%95%8C/</link>
      <pubDate>Sun, 16 Nov 2025 23:14:00 +0800</pubDate>
      <guid>https://www.kingdeguo.com/2025/11/16/%E6%80%9D%E7%BB%B4%E5%AE%9E%E9%AA%8C%E4%BB%8E%E5%B0%BC%E9%87%87%E7%9C%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BE%B9%E7%95%8C/</guid>
      <description>&lt;p&gt;最近几个月，朋友圈里聊得最多的，是&amp;quot;具身智能要来了&amp;quot;、&amp;ldquo;语言模型要到头了&amp;rdquo;、&amp;ldquo;下一波技术革命已经脱离文本了&amp;rdquo;。&lt;/p&gt;&#xA;&lt;p&gt;身在业务，我其实不太关心学术圈今天又发了什么论文，也不是要跟谁在显卡密度上比拼。&lt;/p&gt;&#xA;&lt;p&gt;可是语言模型在企业业务里的表现，确实慢慢露出了&amp;quot;天花板的影子&amp;quot;：越做越多的微调，越堆越大的提示词，越复杂越难的 RAG 管线……效果提升变得极慢，成本却蹭蹭往上走。&lt;/p&gt;&#xA;&lt;p&gt;今年行业里被戏称为 Agent 元年，民间甚至流传一句话：&amp;ldquo;工程优化没啥用了，不如等模型升级。&amp;ldquo;但体感上发现，模型升级仍在发生，但惊艳越来越少，阶跃式提升愈发罕见。&lt;/p&gt;&#xA;&lt;p&gt;就在这种时候，我想起了尼采。这个喜欢把哲学写成炸药包的人，曾经无数次提醒我们：语言不是世界本身，语言只是世界的影子。我们越相信语言，就越容易忘记这一点。&lt;/p&gt;&#xA;&lt;p&gt;于是我想试图用尼采的视角，做一个思维实验：语言模型的边界，是不是语言本身的边界？而当前 AI 的新的探索方向，又是不是在突破语言的围墙？&lt;/p&gt;&#xA;&lt;p&gt;尼采在《论真理与谎言》里说过一句特别好的可以用来解释 LLM 的话：人类在语言里生活得太久，以至于忘了语言只是隐喻，而不是现实本身。这句话如果搬到今天，就是：大语言模型并不是&amp;quot;理解&amp;rdquo;，它只是在影子里跳舞。&lt;/p&gt;&#xA;&lt;p&gt;当然，影子跳得很漂亮，跳得像是理解。但只要你让它做一点跨影子操作——比如把语言映射进真实动作、真实环境、真实物理——它立刻开始露馅。&lt;/p&gt;&#xA;&lt;p&gt;这不是模型不够聪明，而是语言本身的结构在限制它。语言只能描述那些&amp;quot;已经被分类过的经验&amp;rdquo;。而世界运转的方式，往往没那么干净利落——摩擦力、偶然性、模糊性、不可描述的感受、没有名字的现象……语言压根没法完全描述。模型再强，它也是在语言的维度里做插值。这一层楼建得再华丽，楼下那片更大的地基——真实世界——依旧没有被触碰。&lt;/p&gt;&#xA;&lt;p&gt;做业务落地的人都知道一个残酷事实：LLM 很能写，但不太能干。它能生成报告，但不能保证数据精准。它能给你方案，但不能确保流程可执行。它能聊天，但不能对现场复杂性做反应。在结构化、可控、可验证的任务上——模型越大，不一定越稳定。我们像是在用一张&amp;quot;语言滤镜&amp;quot;去看业务流程。很多时候，它能让流程更顺滑；但有些时候，这个滤镜本身就遮住了问题。&lt;/p&gt;&#xA;&lt;p&gt;尼采会提醒我们：&amp;ldquo;你把语言当做真理，本身就是最大的误解。&amp;ldquo;反过来讲：如果语言不是全部，那基于语言的智能，也就不可能是全部。&lt;/p&gt;&#xA;&lt;p&gt;有人马上会举手：不是早就让大模型&amp;quot;动手&amp;quot;了吗？ReAct、Function Call、Tool-use、ChatGPT 插件商店，甚至 AutoGPT，都在把&amp;quot;说&amp;quot;翻译成&amp;quot;做&amp;rdquo;。给模型一个搜索 API，它能自己查资料；给它一个下单接口，它真能帮你买机票。&lt;/p&gt;&#xA;&lt;p&gt;看起来语言模型已经长出胳膊了。但仔细观察，这些&amp;quot;行动&amp;quot;都绕不开两步：第一步，先把工具抽象成一段文本说明书——函数名、参数、返回值，全是语言；第二步，模型依旧只在语言空间里做&amp;quot;下一步该调用哪个函数&amp;quot;的概率选择，真正去拧螺丝、点鼠标、走物理流程的，是外面那层被封装好的小工蜂。&lt;/p&gt;&#xA;&lt;p&gt;换句话说，模型还是站在玻璃后面，用&amp;quot;语言手柄&amp;quot;遥控世界；手柄再长，玻璃还在。&lt;/p&gt;&#xA;&lt;p&gt;世界给出的实时摩擦力、阻尼、意外误差，被 API 的返回值阉割成了几行 JSON，再回传给模型。误差被阉割，反馈就失真，下一轮决策就继续飘在语言层。只要系统里出现 API 没覆盖的异常——快递延迟、网络丢包、机械臂打滑——整条链就断给你看。&lt;/p&gt;&#xA;&lt;p&gt;所以 ReAct 们解决的是&amp;quot;把语言模型连到数字世界的插座&amp;rdquo;，而不是&amp;quot;让模型长出自己的皮肤去蹭真实世界的粗糙&amp;quot;。插座再多，也替代不了肉身。&lt;/p&gt;&#xA;&lt;p&gt;这就回到尼采那句老话：语言只是隐喻。API 也是隐喻，而且是更狡猾的隐喻——它让开发者误以为&amp;quot;已经接地&amp;quot;，其实接的是一层薄薄的语义锡箔，一戳就破。&lt;/p&gt;&#xA;&lt;p&gt;真正难的是下一步：让模型的参数里直接编码&amp;quot;我把杯子摔到地上会碎&amp;quot;的因果，而不是&amp;quot;读到过很多句子说杯子会碎&amp;quot;。前者需要持续的动作–感官–后果闭环，后者只需要文本统计。文本统计再膨胀，也统计不出玻璃碴子的锋利温度。&lt;/p&gt;&#xA;&lt;p&gt;Yann LeCun 已决定近期离职 Meta，计划创业主攻世界模型。这不是更大的语言模型，而是带因果、带时间、带物理、带行动反馈的智能系统——脱离&amp;quot;语言影子&amp;quot;的智能。&lt;/p&gt;&#xA;&lt;p&gt;谷歌的 Genie 3、腾讯 HunyuanWorld-Voyager 与伯克利 LWM 等最新发布，也都在尝试实时交互、环境感知和因果推演，可见业界正形成共识：语言模型的极限，就是语言本身的极限；继续堆大模型，功效比会急剧下降。相比之下，如果让模型去&amp;quot;做&amp;quot;，而不是去&amp;quot;说&amp;quot;，它突然获得了一个全新的坐标系：反馈、试错、物理约束、环境复杂性。这是语言模型永远无法从文本中学到的东西。&lt;/p&gt;&#xA;&lt;p&gt;如果把尼采搬到今天，他很可能会说：&amp;ldquo;语言模型的伟大，在于它把语言榨干了。&amp;rdquo;&lt;/p&gt;&#xA;&lt;p&gt;&amp;ldquo;语言模型的限制，在于它只能榨语言。&amp;ldquo;这句话其实能给研发和产品团队一个很务实的启发——把 LLM 当作语言系统，而不是世界系统。&lt;/p&gt;&#xA;&lt;p&gt;它在抽象思考、文本结构化、文本生成、推理的线性组织、把混乱的信息变得清晰这些地方特别强。但它在面对真实世界的连续反馈、操作性的物理任务、动态、非语言式的信息、完全未知领域的探索这些地方天然弱。&lt;/p&gt;&#xA;&lt;p&gt;当我们给它布置任务时，如果方式是&amp;quot;给我解释&amp;quot;&amp;ldquo;给我总结&amp;quot;&amp;ldquo;给我判断&amp;rdquo;，它表现得像个天才；但只要任务变成&amp;quot;给我行动&amp;quot;&amp;ldquo;给我试错&amp;quot;&amp;ldquo;给我反馈循环&amp;rdquo;，它立刻掉线。这不是模型的问题，是语言的天花板。&lt;/p&gt;&#xA;&lt;p&gt;如果说大语言模型是语言智能的珠峰，那下一座山应该叫&amp;quot;世界智能&amp;rdquo;。&lt;/p&gt;&#xA;&lt;p&gt;世界模型大概会有几个核心要素：不再只靠文本——开始大量引入感知、动作、物理控制的数据；不再只做概率预测——开始建立因果模型，能够解释、预判、修正；不再只在词和词之间运算——会在&amp;quot;行为和后果&amp;quot;之间运算；不再只输出句子——会直接输出行动策略。&lt;/p&gt;&#xA;&lt;p&gt;这听起来像科幻，但实际上已经开始发生。未来的模型不是能把论文写得更好，而是能把机器人动作做得更稳；不是更会讲道理，而是更能在真实世界的混沌里生存。&lt;/p&gt;&#xA;&lt;p&gt;这也意味着另一种边界正在浮现：语言模型不会消失，但会变成&amp;quot;智能系统中的一个模块&amp;rdquo;，就像嘴巴永远还在，但人类并不是靠嘴巴生存。&lt;/p&gt;&#xA;&lt;p&gt;尼采的批判提醒我们，语言的力量很大，但它从来不是全部。大语言模型已经把&amp;quot;语言这个维度&amp;quot;极限地挖掘了一遍，我们看到了它的光辉，也看到了它的尽头。接下来要发生的，是&amp;quot;从语言走向世界&amp;quot;的跳跃——具身、因果、行动、反馈、真实物理世界的不可预测性。这是语言描述不了的，但智能必须面对。&lt;/p&gt;&#xA;&lt;p&gt;在商业世界里，这意味着：语言模型不再是业务全部的解决方案，而是一个高效的子系统。它解决认知层的问题，但执行层、决策层、模型驱动层，必须有新的东西补上。语言像是一束光，照亮了我们理解世界的方式；但世界本身比这束光更辽阔。&lt;/p&gt;&#xA;&lt;p&gt;如果说 LLM 是语言的极限，那下一步，就是开始重新面对世界本身&lt;/p&gt;</description>
    </item>
    <item>
      <title>大模型到底改变了什么？</title>
      <link>https://www.kingdeguo.com/2025/10/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%B0%E5%BA%95%E6%94%B9%E5%8F%98%E4%BA%86%E4%BB%80%E4%B9%88/</link>
      <pubDate>Wed, 01 Oct 2025 17:10:00 +0800</pubDate>
      <guid>https://www.kingdeguo.com/2025/10/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%B0%E5%BA%95%E6%94%B9%E5%8F%98%E4%BA%86%E4%BB%80%E4%B9%88/</guid>
      <description>&lt;h1 id=&#34;大模型到底改变了什么&#34;&gt;大模型到底改变了什么？&lt;/h1&gt;&#xA;&lt;p&gt;最近我常常在想一个问题：大模型到底改变了什么？&lt;/p&gt;&#xA;&lt;p&gt;有时候，我觉得它带来的冲击比表面看到的还要大。以前我们做事，总有一种&amp;quot;节奏感&amp;quot;。比如一个方案要几天准备，一个流程要几周走完，一个文档要一个人慢慢写。时间和空间像是一种自然约束，每个人都被它框住。&lt;/p&gt;&#xA;&lt;p&gt;可大模型出现后，很多节奏突然被打乱了。它能在几分钟里生成一份初稿，能同时处理多个任务，能让原本需要排队、等待、反复确认的事情瞬间推进。以前那种&amp;quot;必须花时间消耗&amp;quot;的逻辑，好像被改写了。&lt;/p&gt;&#xA;&lt;p&gt;这让我想起一句话：尼采说&amp;quot;上帝死了&amp;quot;，意味着旧的秩序崩塌，人必须重新面对世界。而大模型的出现，也像是在提醒我们：很多旧的秩序被抹平了，我们得重新去思考。&lt;/p&gt;&#xA;&lt;p&gt;在工作里，我已经真切感受到这种变化。过去，一个项目提案要靠几个人反复讨论、磨合、再定稿；现在，我自己先丢给模型跑一版资料，立刻就能拿到一个方向，再带着成果和大家讨论，节奏完全不同了。过去，写一封正式的邮件要反复斟酌用词；现在，模型能帮我先列一个框架，我只需要根据场景润色。那种&amp;quot;节奏感&amp;quot;，真的被重构了。&lt;/p&gt;&#xA;&lt;p&gt;我有时候甚至会有点震惊：原本我们以为不可替代的流程、不可压缩的时间，现在突然都变得松动了。这让我不得不重新思考：如果效率不再是瓶颈，那我们该怎么定义自己的价值？如果机器能并行处理一切&amp;quot;可量化&amp;quot;的工作，那我们作为人，还能贡献什么？&lt;/p&gt;&#xA;&lt;p&gt;这些问题我现在还没有答案。但我能确定的是，大模型不是在帮我们省点时间这么简单，而是把我们对&amp;quot;时间、空间和工作节奏&amp;quot;的理解彻底翻了一遍。&lt;/p&gt;&#xA;&lt;p&gt;它让我意识到，我们已经站在一个新的门槛上。很多事情，需要我们重新去看，重新去理解。&lt;/p&gt;</description>
    </item>
    <item>
      <title>员工的 AI 与领导的 AI</title>
      <link>https://www.kingdeguo.com/2025/09/19/%E5%91%98%E5%B7%A5%E7%9A%84-ai-%E4%B8%8E%E9%A2%86%E5%AF%BC%E7%9A%84-ai/</link>
      <pubDate>Fri, 19 Sep 2025 19:24:00 +0800</pubDate>
      <guid>https://www.kingdeguo.com/2025/09/19/%E5%91%98%E5%B7%A5%E7%9A%84-ai-%E4%B8%8E%E9%A2%86%E5%AF%BC%E7%9A%84-ai/</guid>
      <description>&lt;h1 id=&#34;员工的-ai-与领导的-ai&#34;&gt;员工的 AI 与领导的 AI&lt;/h1&gt;&#xA;&lt;p&gt;最近朋友聊起他们团队的情况，讲到 AI 的使用方式，我突然有了新的认知——员工用 AI 和领导用 AI，其实完全不是一回事。员工使用 AI，通常是为了提升个人效率——写文章、写代码、整理资料，它像一把放大镜，让每一项工作更精准、更高效。而领导使用 AI，则更多牵涉组织决策和团队管理，影响更深，也更复杂。&lt;/p&gt;&#xA;&lt;p&gt;朋友描述的一个场景让我印象深刻：领导用 AI 生成了一整套项目方案，直接交给团队执行。表面上看似高效，但团队发现很多内容与实际情况脱节，不得不花额外时间拆解和调整。这让我思考，传统管理方式与 AI 的结合并非天然顺畅。过去的管理强调计划、流程和层级，而 AI 的介入放大了决策的速度，也放大了偏差。当领导依赖 AI 生成指令，却忽略了团队实际执行能力时，组织就可能陷入&amp;quot;表面高效、实则低效&amp;quot;的怪圈。&lt;/p&gt;&#xA;&lt;p&gt;员工 AI 与领导 AI 的差异，也折射出组织运作的核心逻辑。员工 AI 是效率的放大器，让个体工作更精准、可控；领导 AI 则是决策的放大器，它放大的是组织的节奏和方向。如果没有对协作模式、沟通机制和执行能力的深入理解，领导 AI 产生的方案往往难以落地。由此可见，新技术的价值不能脱离组织和协作体系，否则效率提升可能只是表面现象。&lt;/p&gt;&#xA;&lt;p&gt;理性看待 AI 的使用显得尤为重要。员工的 AI 应该作为辅助工具，让个体能力得到放大；领导的 AI 应作为决策参考，让组织节奏更稳健。同时，组织需要在传统管理基础上重构协作方式：明确职责边界、保持沟通畅通、调整反馈机制。只有这样，AI 的加持才能真正转化为团队整体价值。AI 不应成为形式化的指标或考核手段，而应成为增强智慧、支持决策和优化协作的工具。&lt;/p&gt;&#xA;&lt;p&gt;最终，我的思考回到一个核心点：员工的 AI 与领导的 AI 虽然不同，但如果能在组织和协作逻辑下有机结合，就能让效率和弹性同时存在，让团队在快速变化中保持方向感和韧性。正如朋友所说的那些具体场景，如果领导和团队都能理解 AI 的边界和价值，技术带来的改变才会真正扎根，而不是浮在表面。&lt;/p&gt;</description>
    </item>
    <item>
      <title>想法是创造，思考是破坏</title>
      <link>https://www.kingdeguo.com/2025/09/11/%E6%83%B3%E6%B3%95%E6%98%AF%E5%88%9B%E9%80%A0%E6%80%9D%E8%80%83%E6%98%AF%E7%A0%B4%E5%9D%8F/</link>
      <pubDate>Thu, 11 Sep 2025 00:00:00 +0800</pubDate>
      <guid>https://www.kingdeguo.com/2025/09/11/%E6%83%B3%E6%B3%95%E6%98%AF%E5%88%9B%E9%80%A0%E6%80%9D%E8%80%83%E6%98%AF%E7%A0%B4%E5%9D%8F/</guid>
      <description>&lt;h1 id=&#34;想法是创造思考是破坏&#34;&gt;想法是创造，思考是破坏&lt;/h1&gt;&#xA;&lt;p&gt;当我最初听到&amp;quot;想法是创造，思考是破坏&amp;quot;时，下意识觉得它有些偏激。思考怎么会是破坏呢？它不是一直以来都被视为理性与保障吗？可越在软件工程里观察，我越发现这种说法并不夸张，甚至直击核心。&lt;/p&gt;&#xA;&lt;p&gt;想法，是一束火花。它让人兴奋，能瞬间点燃团队的激情，带来新的可能：无论是&amp;quot;上微服务&amp;quot;的豪言，还是&amp;quot;重构一整个模块&amp;quot;的冲动，想法让未来似乎变得触手可及。但火花再亮，如果不经过拆解，就可能点燃的是一堆干草——瞬间燃烧，最终只剩一地灰烬。&lt;/p&gt;&#xA;&lt;p&gt;思考，恰恰是那个看似冷酷的&amp;quot;破坏者&amp;quot;。它不带激情，而是拿着锤子，把闪闪发光的创意逐块敲开，逼我们回答最难听的问题：十倍的流量来了，架构是否能扛住？团队翻倍后，流程是否还能运转？未来方向变了，今天的设计还能存活吗？这种拆解确实残酷，它像是不断推倒沙盘上的模型。但也正是这种破坏，才筛掉了华而不实的幻想，留下了真正能经得住时间考验的方案。&lt;/p&gt;&#xA;&lt;p&gt;走到这里，我对&amp;quot;创造&amp;quot;和&amp;quot;破坏&amp;quot;的理解发生了转变。创造，不只是提出一个点子，而是敢于把未来打开，让可能性出现；破坏，也不是真正意义上的摧毁，而是一种净化，把脆弱和虚幻剔除出去。两者并不是对立，而是互为因果：没有创造，破坏就成了空洞的挑剔；没有破坏，创造也会淹没在复杂度与债务中。&lt;/p&gt;&#xA;&lt;p&gt;所以我慢慢明白，创造和破坏其实是一体的。创造让未来展开，给我们勇气去尝试；破坏让幻想收缩，逼我们留下真实能走下去的部分。就像在软件工程里，一个健康的系统不是靠一连串灵感堆出来的，而是靠无数次推倒、重来、取舍之后，才沉淀成可靠的架构。激情与冷静、点子与质疑，本就该交替出现，这样滚动向前，才算是真正的进化。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
