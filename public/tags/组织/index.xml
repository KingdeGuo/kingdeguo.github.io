<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>组织 on kingdeguo&#39;s blog</title>
    <link>https://www.kingdeguo.com/tags/%E7%BB%84%E7%BB%87/</link>
    <description>Recent content in 组织 on kingdeguo&#39;s blog</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 24 Jan 2026 11:10:00 +0800</lastBuildDate>
    <atom:link href="https://www.kingdeguo.com/tags/%E7%BB%84%E7%BB%87/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>看起来正确的事情与不得不做的事情</title>
      <link>https://www.kingdeguo.com/2026/01/24/%E7%9C%8B%E8%B5%B7%E6%9D%A5%E6%AD%A3%E7%A1%AE%E7%9A%84%E4%BA%8B%E6%83%85%E4%B8%8E%E4%B8%8D%E5%BE%97%E4%B8%8D%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85/</link>
      <pubDate>Sat, 24 Jan 2026 11:10:00 +0800</pubDate>
      <guid>https://www.kingdeguo.com/2026/01/24/%E7%9C%8B%E8%B5%B7%E6%9D%A5%E6%AD%A3%E7%A1%AE%E7%9A%84%E4%BA%8B%E6%83%85%E4%B8%8E%E4%B8%8D%E5%BE%97%E4%B8%8D%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85/</guid>
      <description>&lt;p&gt;很多年以后再回头看，我越来越确定一件事：大多数管理决策失败，并不是因为判断力不足，而是因为我们在错误的层级上讨论了“正确”。&lt;/p&gt;&#xA;&lt;p&gt;在公司内部，“看起来正确”的事情通常拥有完整的逻辑闭环。它站在长期价值、战略一致性、用户体验、组织健康这些高位概念上，任何一条单独拿出来都很难反驳。更重要的是，它往往能被清晰表达、被多数人理解、被记录进PPT和复盘文档。&lt;/p&gt;&#xA;&lt;p&gt;但现实的问题在于：组织并不是运行在逻辑层面，而是运行在约束之中。&lt;/p&gt;&#xA;&lt;p&gt;现金流、节奏窗口、组织能力、关键岗位成熟度、外部竞争强度，这些变量并不会因为你“方向正确”而自动让路。它们像重力一样存在，不讲道理，却决定运动轨迹。&lt;/p&gt;&#xA;&lt;p&gt;于是，“不得不做的事情”出现了。&lt;/p&gt;&#xA;&lt;p&gt;它通常不完整、不优雅，甚至在单点上是错误的。它可能牺牲体验换效率，牺牲长期结构换短期确定性，牺牲一致性换生存概率。你很清楚这不是理想解，但你也同样清楚：如果不这么做，系统会在别的地方先塌。&lt;/p&gt;&#xA;&lt;p&gt;真正折磨人的地方在于，这两类判断并不发生在不同的人身上，而是同时发生在同一个管理者脑子里。&lt;/p&gt;&#xA;&lt;p&gt;你能同时看到理想路径，也能同时感知现实压力。这不是信息不对称，而是认知冲突。&lt;/p&gt;&#xA;&lt;p&gt;经验尚浅的时候，人往往会用“看起来正确”来对抗不安。因为它是可解释的、可被认同的，也更容易获得组织层面的支持。即便结果不佳，也能被归因为执行问题、环境问题、运气问题。&lt;/p&gt;&#xA;&lt;p&gt;而真正进入复杂阶段后，你会发现，有些决策一旦走到“不得不做”，其实已经没有退路了。它不是你主动选择的方向，而是系统把你推到那里的。&lt;/p&gt;&#xA;&lt;p&gt;这里有一个极其关键、却很少被点破的事实：&lt;/p&gt;&#xA;&lt;p&gt;管理决策的本质，并不是价值选择，而是系统稳定性选择。&lt;/p&gt;&#xA;&lt;p&gt;当系统还稳定时，你可以讨论价值、原则、理想状态；当系统开始失稳时，所有讨论都会自动降级为“如何避免崩溃”。&lt;/p&gt;&#xA;&lt;p&gt;很多看起来“背离初心”的决策，真实动机并不是短视，而是对系统脆弱点的直觉判断。只不过，这种判断往往无法被完整表达，只能被执行。&lt;/p&gt;&#xA;&lt;p&gt;真正危险的，不是做了不得不做的事，而是在事后失去了对这件事的真实定义权。&lt;/p&gt;&#xA;&lt;p&gt;一旦你开始用长期逻辑为一次短期权宜辩护，用战略语言包装一次风险对冲，那条边界就被抹掉了。组织也会从“阶段性偏航”，悄然滑向“路径依赖”。&lt;/p&gt;&#xA;&lt;p&gt;这是很多公司真正走偏的起点：不是犯错，而是误判了错误的性质。&lt;/p&gt;&#xA;&lt;p&gt;成熟的管理者，往往会在内部保留一套“真实账本”。对外，他可以给出足够体面的叙事；对内，他必须清楚知道：这一步是补丁，不是方向；是缓冲，不是升级；是为了活下去，而不是为了证明我们是对的。&lt;/p&gt;&#xA;&lt;p&gt;你会发现，真正厉害的管理者，很少沉迷于证明自己判断的正确性。他们更在意的是：这次决策，会不会改变组织未来做决策的方式。&lt;/p&gt;&#xA;&lt;p&gt;如果一次“不得不做”，让团队误以为这是新范式；如果一次应急解法，被当成成功经验复制；那这次决策的真实成本，远高于当初看到的收益。&lt;/p&gt;&#xA;&lt;p&gt;所以，所谓“清醒”，并不是永远坚持看起来正确的事，而是在被迫妥协时，仍然保持对长期结构的敬畏感。&lt;/p&gt;&#xA;&lt;p&gt;你可以暂时牺牲效率之外的东西，但要清楚哪些东西一旦牺牲，就很难再拿回来；你可以延后理想，但不能重写理想而不自知。&lt;/p&gt;&#xA;&lt;p&gt;管理的难，从来不在选择题本身，而在于你是否知道：自己此刻是在低头过门槛，还是已经换了一条路。&lt;/p&gt;&#xA;&lt;p&gt;很多年后拉开差距的，也正是这一点。&lt;/p&gt;&#xA;&lt;p&gt;不是谁更聪明，而是谁在“不得不做”的时候，还没有把“看起来正确”彻底忘掉。&lt;/p&gt;</description>
    </item>
    <item>
      <title>员工的 AI 与领导的 AI</title>
      <link>https://www.kingdeguo.com/2025/09/19/%E5%91%98%E5%B7%A5%E7%9A%84-ai-%E4%B8%8E%E9%A2%86%E5%AF%BC%E7%9A%84-ai/</link>
      <pubDate>Fri, 19 Sep 2025 19:24:00 +0800</pubDate>
      <guid>https://www.kingdeguo.com/2025/09/19/%E5%91%98%E5%B7%A5%E7%9A%84-ai-%E4%B8%8E%E9%A2%86%E5%AF%BC%E7%9A%84-ai/</guid>
      <description>&lt;h1 id=&#34;员工的-ai-与领导的-ai&#34;&gt;员工的 AI 与领导的 AI&lt;/h1&gt;&#xA;&lt;p&gt;最近朋友聊起他们团队的情况，讲到 AI 的使用方式，我突然有了新的认知——员工用 AI 和领导用 AI，其实完全不是一回事。员工使用 AI，通常是为了提升个人效率——写文章、写代码、整理资料，它像一把放大镜，让每一项工作更精准、更高效。而领导使用 AI，则更多牵涉组织决策和团队管理，影响更深，也更复杂。&lt;/p&gt;&#xA;&lt;p&gt;朋友描述的一个场景让我印象深刻：领导用 AI 生成了一整套项目方案，直接交给团队执行。表面上看似高效，但团队发现很多内容与实际情况脱节，不得不花额外时间拆解和调整。这让我思考，传统管理方式与 AI 的结合并非天然顺畅。过去的管理强调计划、流程和层级，而 AI 的介入放大了决策的速度，也放大了偏差。当领导依赖 AI 生成指令，却忽略了团队实际执行能力时，组织就可能陷入&amp;quot;表面高效、实则低效&amp;quot;的怪圈。&lt;/p&gt;&#xA;&lt;p&gt;员工 AI 与领导 AI 的差异，也折射出组织运作的核心逻辑。员工 AI 是效率的放大器，让个体工作更精准、可控；领导 AI 则是决策的放大器，它放大的是组织的节奏和方向。如果没有对协作模式、沟通机制和执行能力的深入理解，领导 AI 产生的方案往往难以落地。由此可见，新技术的价值不能脱离组织和协作体系，否则效率提升可能只是表面现象。&lt;/p&gt;&#xA;&lt;p&gt;理性看待 AI 的使用显得尤为重要。员工的 AI 应该作为辅助工具，让个体能力得到放大；领导的 AI 应作为决策参考，让组织节奏更稳健。同时，组织需要在传统管理基础上重构协作方式：明确职责边界、保持沟通畅通、调整反馈机制。只有这样，AI 的加持才能真正转化为团队整体价值。AI 不应成为形式化的指标或考核手段，而应成为增强智慧、支持决策和优化协作的工具。&lt;/p&gt;&#xA;&lt;p&gt;最终，我的思考回到一个核心点：员工的 AI 与领导的 AI 虽然不同，但如果能在组织和协作逻辑下有机结合，就能让效率和弹性同时存在，让团队在快速变化中保持方向感和韧性。正如朋友所说的那些具体场景，如果领导和团队都能理解 AI 的边界和价值，技术带来的改变才会真正扎根，而不是浮在表面。&lt;/p&gt;</description>
    </item>
    <item>
      <title>用测评把大模型从黑盒带回组织的理性边界</title>
      <link>https://www.kingdeguo.com/2025/01/25/%E7%94%A8%E6%B5%8B%E8%AF%84%E6%8A%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8E%E9%BB%91%E7%9B%92%E5%B8%A6%E5%9B%9E%E7%BB%84%E7%BB%87%E7%9A%84%E7%90%86%E6%80%A7%E8%BE%B9%E7%95%8C/</link>
      <pubDate>Sat, 25 Jan 2025 10:19:00 +0800</pubDate>
      <guid>https://www.kingdeguo.com/2025/01/25/%E7%94%A8%E6%B5%8B%E8%AF%84%E6%8A%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8E%E9%BB%91%E7%9B%92%E5%B8%A6%E5%9B%9E%E7%BB%84%E7%BB%87%E7%9A%84%E7%90%86%E6%80%A7%E8%BE%B9%E7%95%8C/</guid>
      <description>&lt;p&gt;在传统系统里，我们对“效果可预期”这件事有着天然的安全感。&lt;/p&gt;&#xA;&lt;p&gt;规则可能复杂，但它们始终是规则：输入是什么，经过哪些判断，最终输出什么，大体是可以被穷举、被回溯、被解释的。业务人员心里有数，技术人员也能兜底。系统未必聪明，但它是“透明”的。&lt;/p&gt;&#xA;&lt;p&gt;大模型进入组织之后，这种安全感开始松动。不是因为它不工作，而是因为它“看起来什么都会做，却说不清为什么这么做”。同样的输入，可能得到略有差异的输出；同样的任务，在不同语境下呈现出不同判断。这种不稳定并不一定是坏事，但它打破了组织对系统的一项核心预期：可预知性。&lt;/p&gt;&#xA;&lt;p&gt;于是，AI 测评的价值并不在于“给模型打分”，而在于把这种不确定性重新拉回到可感知、可讨论、可管理的范围内。&lt;/p&gt;&#xA;&lt;p&gt;测评不是为了证明模型有多聪明，而是为了回答一个更现实的问题：在我们设定的边界内，它会如何表现，它的表现是否稳定，以及这种稳定性是否足以支撑业务使用。&lt;/p&gt;&#xA;&lt;p&gt;从这个角度看，测评的本质是在做一件很朴素的事——把黑盒拆解成组织能理解的白盒。哪怕我们无法完全解释模型内部的推理路径，至少可以通过系统化的测评，让团队知道它在什么条件下可靠，在什么条件下会偏离预期。不是“信不信 AI”，而是“信到什么程度、信在哪些场景”。&lt;/p&gt;&#xA;&lt;p&gt;这也解释了一个容易被误解的点：在大多数组织里，AI 并不是一个“决策者”，而更像是一个被约束的劳工。它确实在执行过程中做了价值判断，但这些判断发生在预先设定的规则、目标和评价体系之内。测评的意义，正是确保这些判断始终被关在围栏里。&lt;/p&gt;&#xA;&lt;p&gt;如果 AI 仅仅被当作纯工具——比如生成草稿、做信息整理、提高效率——那么测评的要求其实并不高。偶尔不稳定、偶尔跑偏，顶多是效率损失。但一旦 AI 被引入到更接近决策的位置，比如影响审批、推荐路径、资源分配，那么问题就完全不同了。此时，测评不再是“优化体验”的手段，而是进入组织决策体系的门票。&lt;/p&gt;&#xA;&lt;p&gt;从这个意义上说，AI 测评并不是在限制创新，而是在为规模化使用创造条件。没有测评，AI 只能停留在个人工具层面；有了测评，它才有可能成为组织级能力。前者依赖个人判断，后者依赖共识，而共识的前提，永远是可被反复验证的稳定效果。&lt;/p&gt;&#xA;&lt;p&gt;所以，这件事看起来像是在“给 AI 加枷锁”，但实际上是在为组织保留对系统的控制权。不是让模型替我们思考，而是确保当它替我们干活时，我们始终知道它大概会怎么干、可能在哪里出问题、出了问题该由谁负责。&lt;/p&gt;&#xA;&lt;p&gt;当黑盒被一点点照亮，AI 才不再是一种令人不安的能力放大器，而是一个可以被信任、被托付、被纳入流程的基础设施。这正是 AI 测评真正的作用所在。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
