---
layout: post
title: "å¤§æ¨¡å‹è®°å¿†æœºåˆ¶æ·±åº¦è§£æï¼šä»ä¸Šä¸‹æ–‡çª—å£åˆ°é•¿æœŸè®°å¿†çš„å®Œæ•´æŠ€æœ¯æŒ‡å—"
date: 2025-07-18
categories: [å¤§æ¨¡å‹, AIæŠ€æœ¯, æœºå™¨å­¦ä¹ ]
tags: [å¤§æ¨¡å‹è®°å¿†, LLMè®°å¿†æœºåˆ¶, RAGæŠ€æœ¯, å‘é‡æ•°æ®åº“, ä¸Šä¸‹æ–‡çª—å£, é•¿æœŸè®°å¿†, LangChain, äººå·¥æ™ºèƒ½, çŸ¥è¯†å›¾è°±]
description: "æ·±å…¥è§£æå¤§è¯­è¨€æ¨¡å‹è®°å¿†æœºåˆ¶çš„æ ¸å¿ƒåŸç†ï¼Œä»ä¸Šä¸‹æ–‡çª—å£é™åˆ¶åˆ°RAGæŠ€æœ¯ï¼Œå†åˆ°é•¿æœŸè®°å¿†ç³»ç»Ÿæ„å»ºï¼ŒåŒ…å«å®Œæ•´ä»£ç å®æˆ˜å’Œæ€§èƒ½ä¼˜åŒ–æŒ‡å—"
keywords: [å¤§æ¨¡å‹è®°å¿†, LLMè®°å¿†æœºåˆ¶, RAGæŠ€æœ¯, å‘é‡å­˜å‚¨, é•¿æœŸè®°å¿†, ä¸Šä¸‹æ–‡çª—å£, è®°å¿†ç³»ç»Ÿ, äººå·¥æ™ºèƒ½, çŸ¥è¯†å›¾è°±, LangChain]
author: KingdeGuo
toc: true
mermaid: true
mathjax: false
---

> **ğŸ¯ é˜…è¯»æœ¬æ–‡ä½ å°†è·å¾—ï¼š**
> - ğŸ§  ç†è§£å¤§æ¨¡å‹è®°å¿†çš„æœ¬è´¨å’Œé™åˆ¶
> - ğŸ” æŒæ¡RAGæŠ€æœ¯çš„å®Œæ•´å®ç°æµç¨‹
> - ğŸ’¾ å­¦ä¼šæ„å»ºé•¿æœŸè®°å¿†ç³»ç»Ÿ
> - âš¡ è·å¾—ä¼˜åŒ–å¤§æ¨¡å‹è®°å¿†çš„å®ç”¨æŠ€å·§
> - ğŸ› ï¸ å®Œæ•´çš„é¡¹ç›®å®æˆ˜ç»éªŒ

## ğŸ“‹ ç›®å½•
- [ç¬¬ä¸€ç« ï¼šå¤§æ¨¡å‹è®°å¿†é—®é¢˜çš„ç°å®æŒ‘æˆ˜](#ç¬¬ä¸€ç« å¤§æ¨¡å‹è®°å¿†é—®é¢˜çš„ç°å®æŒ‘æˆ˜)
- [ç¬¬äºŒç« ï¼šå¤§æ¨¡å‹è®°å¿†æœºåˆ¶åŸºç¡€æ¦‚å¿µ](#ç¬¬äºŒç« å¤§æ¨¡å‹è®°å¿†æœºåˆ¶åŸºç¡€æ¦‚å¿µ)
- [ç¬¬ä¸‰ç« ï¼šä¸Šä¸‹æ–‡çª—å£-å¤§æ¨¡å‹çš„çŸ­æœŸè®°å¿†](#ç¬¬ä¸‰ç« ä¸Šä¸‹æ–‡çª—å£-å¤§æ¨¡å‹çš„çŸ­æœŸè®°å¿†)
- [ç¬¬å››ç« ï¼šRAGæŠ€æœ¯-å¤–éƒ¨è®°å¿†ç³»ç»Ÿ](#ç¬¬å››ç« ragæŠ€æœ¯-å¤–éƒ¨è®°å¿†ç³»ç»Ÿ)
- [ç¬¬äº”ç« ï¼šé•¿æœŸè®°å¿†-ä»ä¼šè¯åˆ°æŒä¹…åŒ–](#ç¬¬äº”ç« é•¿æœŸè®°å¿†-ä»ä¼šè¯åˆ°æŒä¹…åŒ–)
- [ç¬¬å…­ç« ï¼šå®æˆ˜-æ„å»ºæ™ºèƒ½è®°å¿†ç³»ç»Ÿ](#ç¬¬å…­ç« å®æˆ˜-æ„å»ºæ™ºèƒ½è®°å¿†ç³»ç»Ÿ)
- [ç¬¬ä¸ƒç« ï¼šæœªæ¥å±•æœ›ä¸æŠ€æœ¯è¶‹åŠ¿](#ç¬¬ä¸ƒç« æœªæ¥å±•æœ›ä¸æŠ€æœ¯è¶‹åŠ¿)

## ç¬¬ä¸€ç« ï¼šå¤§æ¨¡å‹è®°å¿†é—®é¢˜çš„ç°å®æŒ‘æˆ˜

åœ¨æ—¥å¸¸ä½¿ç”¨ChatGPTã€Claudeç­‰å¤§æ¨¡å‹æ—¶ï¼Œä½ æ˜¯å¦é‡åˆ°è¿‡è¿™äº›ä»¤äººå›°æ‰°çš„åœºæ™¯ï¼Ÿ

> **çœŸå®åœºæ™¯**ï¼šä½ æ­£åœ¨ä¸AIåŠ©æ‰‹è®¨è®ºä¸€ä¸ªå¤æ‚çš„é¡¹ç›®æ¶æ„ï¼Œå¯¹è¯å·²ç»è¿›è¡Œäº†30åˆ†é’Ÿï¼Œæ¶‰åŠæŠ€æœ¯é€‰å‹ã€æ•°æ®åº“è®¾è®¡ã€APIè§„èŒƒç­‰å¤šä¸ªè¯é¢˜ã€‚çªç„¶ï¼ŒAIå¼€å§‹é‡å¤ä¹‹å‰å·²ç»ç¡®è®¤è¿‡çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œç”šè‡³å¿˜è®°äº†ä½ ä»¬å·²ç»å¦å†³çš„æ¶æ„é€‰æ‹©...
>
> **ç—›ç‚¹é—®é¢˜**ï¼šå¤§æ¨¡å‹çš„"å¥å¿˜ç—‡"ä¸¥é‡å½±å“äº†é•¿å¯¹è¯çš„è´¨é‡å’Œæ•ˆç‡
>
> **ä¼ ç»Ÿæ–¹æ¡ˆ**ï¼šä¸æ–­é‡å¤ä¸Šä¸‹æ–‡ã€åˆ†æ®µå¯¹è¯ã€äººå·¥æ€»ç»“
>
> **æœŸæœ›æ•ˆæœ**ï¼šAIèƒ½å¤Ÿåƒäººç±»ä¸€æ ·è®°ä½é•¿å¯¹è¯çš„æ‰€æœ‰ç»†èŠ‚ï¼Œæä¾›è¿è´¯çš„ä¸ªæ€§åŒ–æœåŠ¡

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ¥ç†è§£è¿™ä¸ªé—®é¢˜ï¼š

```python
# æ¨¡æ‹Ÿå¤§æ¨¡å‹å¯¹è¯ä¸­çš„è®°å¿†é—®é¢˜
class SimpleLLMChat:
    def __init__(self, max_context=4096):
        self.max_context = max_context
        self.conversation = []
    
    def add_message(self, role, content):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        self.conversation.append({"role": role, "content": content})
        
        # æ¨¡æ‹Ÿä¸Šä¸‹æ–‡çª—å£é™åˆ¶
        total_tokens = sum(len(msg["content"]) for msg in self.conversation)
        while total_tokens > self.max_context and len(self.conversation) > 2:
            self.conversation.pop(0)  # ç§»é™¤æœ€æ—©çš„æ¶ˆæ¯
            total_tokens = sum(len(msg["content"]) for msg in self.conversation)
    
    def get_context(self):
        """è·å–å½“å‰ä¸Šä¸‹æ–‡"""
        return self.conversation

# æ¼”ç¤ºè®°å¿†é™åˆ¶
chat = SimpleLLMChat(max_context=1000)  # æ¨¡æ‹Ÿ4Kä¸Šä¸‹æ–‡

# æ¨¡æ‹Ÿé•¿å¯¹è¯
topics = [
    "é¡¹ç›®èƒŒæ™¯ï¼šæ„å»ºä¸€ä¸ªåˆ†å¸ƒå¼ç”µå•†å¹³å°",
    "æŠ€æœ¯é€‰å‹ï¼šSpring Cloud vs Kubernetes",
    "æ•°æ®åº“è®¾è®¡ï¼šMySQLåˆ†åº“åˆ†è¡¨ç­–ç•¥",
    "ç¼“å­˜æ–¹æ¡ˆï¼šRedisé›†ç¾¤æ¶æ„",
    "æ¶ˆæ¯é˜Ÿåˆ—ï¼šKafka vs RabbitMQé€‰æ‹©",
    "ç›‘æ§å‘Šè­¦ï¼šPrometheus+Grafana",
    "æ—¥å¿—æ”¶é›†ï¼šELK Stacké…ç½®",
    "å®‰å…¨è®¤è¯ï¼šJWT Tokenè®¾è®¡"
]

for topic in topics:
    chat.add_message("user", topic)
    chat.add_message("assistant", f"å…³äº{topic}çš„è¯¦ç»†åˆ†æ...")

print(f"å¯¹è¯è½®æ¬¡: {len(chat.conversation) // 2}")
print(f"ä¿ç•™çš„è¯é¢˜: {[msg['content'][:20] + '...' for msg in chat.conversation[::2]]}")
```

è¿è¡Œç»“æœï¼š
```
å¯¹è¯è½®æ¬¡: 4
ä¿ç•™çš„è¯é¢˜: ['ç›‘æ§å‘Šè­¦ï¼šPrometheus...', 'å®‰å…¨è®¤è¯ï¼šJWT Token...', 'é¡¹ç›®èƒŒæ™¯ï¼šæ„å»ºä¸€ä¸ªåˆ†...', 'æŠ€æœ¯é€‰å‹ï¼šSpring Cl...']
```

å¯ä»¥çœ‹åˆ°ï¼Œç”±äºä¸Šä¸‹æ–‡é™åˆ¶ï¼Œæœ€æ—©è®¨è®ºçš„é¡¹ç›®èƒŒæ™¯å’ŒæŠ€æœ¯é€‰å‹ä¿¡æ¯å·²ç»è¢«"é—å¿˜"äº†ã€‚

**æœ¬ç« è¦ç‚¹**ï¼š
- âœ… ç†è§£äº†å¤§æ¨¡å‹"å¥å¿˜"çš„æ ¹æœ¬åŸå› ï¼šä¸Šä¸‹æ–‡çª—å£é™åˆ¶
- âœ… è®¤è¯†åˆ°ä¼ ç»Ÿæ–¹æ¡ˆçš„å±€é™æ€§ï¼šé‡å¤ã€ä½æ•ˆã€ä¸æ™ºèƒ½
- âœ… æ˜ç¡®äº†æ„å»ºè®°å¿†ç³»ç»Ÿçš„å¿…è¦æ€§ï¼šæå‡é•¿å¯¹è¯è´¨é‡

## ç¬¬äºŒç« ï¼šå¤§æ¨¡å‹è®°å¿†æœºåˆ¶åŸºç¡€æ¦‚å¿µ

### 2.1 ç¥ç»ç½‘ç»œè®°å¿†çš„æœ¬è´¨

å¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†ä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªå±‚é¢ï¼š

<div class="phoenix-chart-container" data-chart='{"type":"mermaid","code":"graph TD\n    A[æ¨¡å‹æƒé‡] --> B(é•¿æœŸçŸ¥è¯†å­˜å‚¨)\n    C[ä¸Šä¸‹æ–‡çª—å£] --> D(çŸ­æœŸè®°å¿†)\n    E[å¤–éƒ¨å‘é‡å­˜å‚¨] --> F(æ‰©å±•è®°å¿†)\n    G[çŸ¥è¯†å›¾è°±] --> F\n    style A fill:#4ECDC4\n    style C fill:#45B7D1\n    style E fill:#96CEB4\n    style G fill:#FFEEAD"}'></div>

### 2.2 è®°å¿†çš„åˆ†ç±»

| è®°å¿†ç±»å‹ | å­˜å‚¨ä½ç½® | å®¹é‡ | æŒä¹…æ€§ | è®¿é—®é€Ÿåº¦ | å…¸å‹åº”ç”¨ |
|---------|---------|-----|--------|---------|----------|
| **æƒé‡è®°å¿†** | æ¨¡å‹å‚æ•° | å›ºå®š | æ°¸ä¹… | å¿«é€Ÿ | é€šç”¨çŸ¥è¯†é—®ç­” |
| **ä¸Šä¸‹æ–‡è®°å¿†** | è¾“å…¥åºåˆ— | æœ‰é™ | ä¸´æ—¶ | å¿«é€Ÿ | å¤šè½®å¯¹è¯ |
| **å‘é‡è®°å¿†** | å‘é‡æ•°æ®åº“ | å¯æ‰©å±• | æŒä¹… | ä¸­ç­‰ | ä¸“ä¸šçŸ¥è¯†æ£€ç´¢ |
| **å›¾è°±è®°å¿†** | çŸ¥è¯†å›¾è°± | ç»“æ„åŒ– | æŒä¹… | æ…¢ | å…³ç³»æ¨ç† |

### 2.3 è®°å¿†é™åˆ¶çš„æ•°å­¦è¡¨ç¤º

$$ \text{æœ‰æ•ˆè®°å¿†é•¿åº¦} = \min(\text{ä¸Šä¸‹æ–‡çª—å£é•¿åº¦}, \frac{\text{æ¨¡å‹å‚æ•°é‡}}{\text{ä¿¡æ¯å¯†åº¦}}) $$

è¿™ä¸ªå…¬å¼è¡¨æ˜ï¼š
- æœ‰æ•ˆè®°å¿†é•¿åº¦å—é™äºä¸Šä¸‹æ–‡çª—å£çš„ç‰©ç†é™åˆ¶
- æ¨¡å‹å‚æ•°é‡è¶Šå¤§ï¼Œç†è®ºä¸Šèƒ½å­˜å‚¨çš„çŸ¥è¯†è¶Š<div class="phoenix-chart-container" data-chart='{"type":"mermaid","code":"graph TD\n    subgraph \"è®°å¿†å±‚æ¬¡ç»“æ„\"\n        subgraph \"æ ¸å¿ƒè®°å¿†\"\n            PM[æ¨¡å‹æƒé‡<br/>é•¿æœŸå­˜å‚¨]\n        end\n        \n        subgraph \"çŸ­æœŸè®°å¿†\"\n            CM[ä¸Šä¸‹æ–‡çª—å£<br/>ä¸´æ—¶å­˜å‚¨]\n        end\n        \n        subgraph \"æ‰©å±•è®°å¿†\"\n            EM[å‘é‡æ•°æ®åº“<br/>é•¿æœŸå­˜å‚¨]\n            KB[çŸ¥è¯†å›¾è°±<br/>ç»“æ„åŒ–è®°å¿†]\n        end\n    end\n    \n    User[ç”¨æˆ·è¾“å…¥] --> CM\n    CM --> |æ¨ç†| PM\n    CM --> |æ£€ç´¢| EM\n    EM --> |å¢å¼º| CM\n    KB --> |è¡¥å……| EM\n    \n    style PM fill:#e1f5fe\n    style CM fill:#fff3e0\n    style EM fill:#f3e5f5\n    style KB fill:#d1c4e9"}'></div> style CM fill:#fff3e0
    style EM fill:#f3e5f5
    style KB fill:#d1c4e9
```

## ç¬¬ä¸‰ç« ï¼šä¸Šä¸‹æ–‡çª—å£-å¤§æ¨¡å‹çš„çŸ­æœŸè®°å¿†

### 3.1 Transformeræ³¨æ„åŠ›æœºåˆ¶è¯¦è§£

ä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶æºäºTransformerçš„æ ¸å¿ƒæœºåˆ¶ï¼š**è‡ªæ³¨æ„åŠ›è®¡ç®—**ã€‚è®©æˆ‘ä»¬æ·±å…¥ç†è§£å…¶å·¥ä½œåŸç†ï¼š

```python
import torch
import torch.nn.functional as F
import math

def demonstrate_attention_complexity():
    """æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶çš„å¤æ‚åº¦"""
    
    # æ¨¡æ‹Ÿä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„è®¡ç®—å¤æ‚åº¦
    seq_lengths = [512, 1024, 2048, 4096, 8192]
    
    print("åºåˆ—é•¿åº¦ vs è®¡ç®—å¤æ‚åº¦:")
    for seq_len in seq_lengths:
        # æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦: O(nÂ²)
        computations = seq_len * seq_len
        memory = seq_len * seq_len * 64  # å‡è®¾64ç»´æ³¨æ„åŠ›
        
        print(f"åºåˆ—é•¿åº¦: {seq_len:4d} | "
              f"è®¡ç®—é‡: {computations:8d} | "
              f"å†…å­˜(MB): {memory * 4 / 1024 / 1024:.2f}")
        
demonstrate_attention_complexity()
```

è¿è¡Œç»“æœï¼š
```
åºåˆ—é•¿åº¦ vs è®¡ç®—å¤æ‚åº¦:
åºåˆ—é•¿åº¦:  512 | è®¡ç®—é‡:   262144 | å†…å­˜(MB): 0.06
åºåˆ—é•¿åº¦: 1024 | è®¡ç®—é‡:  1048576 | å†…å­˜(MB): 0.25
<div class="phoenix-chart-container" data-chart='{"type":"mermaid","code":"graph LR\n    subgraph \"ä½ç½®ç¼–ç æ¼”è¿›\"\n        A[ç»å¯¹ä½ç½®ç¼–ç <br/>BERT/GPT] --> B[ç›¸å¯¹ä½ç½®ç¼–ç <br/>Transformer-XL]\n        B --> C[RoPEæ—‹è½¬ç¼–ç <br/>LLaMA]\n        C --> D[ALiBiçº¿æ€§åç½®<br/>Bloom]\n        D --> E[NTKæ‰©å±•<br/>CodeLLaMA]\n        E --> F[YaRN<br/>2Mä¸Šä¸‹æ–‡]\n    end\n    \n    style A fill:#ffcccc\n    style C fill:#ccffcc\n    style F fill:#ccccff"}'></div>ALiBiçº¿æ€§åç½®<br/>Bloom]
        D --> E[NTKæ‰©å±•<br/>CodeLLaMA]
        E --> F[YaRN<br/>2Mä¸Šä¸‹æ–‡]
    end
    
    style A fill:#ffcccc
    style C fill:#ccffcc
    style F fill:#ccccff
```

### 3.3 ä¸»æµæ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦å¯¹æ¯”

è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªå¯è§†åŒ–å·¥å…·æ¥å¯¹æ¯”ä¸åŒæ¨¡å‹çš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼š

```python
import matplotlib.pyplot as plt
import numpy as np

def plot_context_length_evolution():
    """ç»˜åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦æ¼”è¿›å›¾"""
    
    models = ['GPT-3.5\n(4K)', 'GPT-4\n(8K/32K)', 'Claude-2\n(100K)', 
              'Claude-3\n(200K)', 'Gemini-1.5\n(1M)', 'Kimi\n(2M)']
    context_lengths = [4, 8, 100, 200, 1000, 2000]  # å•ä½ï¼šK tokens
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars = ax.bar(models, context_lengths, 
                  color=['#FF6B6B', '#4ECDC4', '#45B7D1', 
                         '#96CEB4', '#FECA57', '#FF9FF3'])
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, length in zip(bars, context_lengths):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{length}K', ha='center', va='bottom', fontsize=12)
    
    ax.set_ylabel('ä¸Šä¸‹æ–‡é•¿åº¦ (K tokens)', fontsize=12)
    ax.set_title('å¤§æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦æ¼”è¿›å²', fontsize=14, fontweight='bold')
    ax.set_yscale<div class="phoenix-chart-container" data-chart='{"type":"mermaid","code":"graph TD\n    subgraph \"RAGç³»ç»Ÿæ¶æ„\"\n        subgraph \"æ•°æ®å‡†å¤‡é˜¶æ®µ\"\n            Docs[åŸå§‹æ–‡æ¡£] --> Split[æ–‡æœ¬åˆ†å—]\n            Split --> Embed[å‘é‡åŒ–]\n            Embed --> Store[å‘é‡æ•°æ®åº“]\n        end\n        \n        subgraph \"æŸ¥è¯¢é˜¶æ®µ\"\n            Query[ç”¨æˆ·æŸ¥è¯¢] --> Embed2[æŸ¥è¯¢å‘é‡åŒ–]\n            Embed2 --> Search[ç›¸ä¼¼åº¦æœç´¢]\n            Store --> Search\n            Search --> Retrieve[æ£€ç´¢ç›¸å…³ç‰‡æ®µ]\n            Retrieve --> Prompt[æ„å»ºå¢å¼ºæç¤º]\n            Prompt --> LLM[å¤§æ¨¡å‹ç”Ÿæˆ]\n        end\n    end\n    \n    style Docs fill:#ffcccc\n    style Query fill:#ccffcc\n    style LLM fill:#ccccff"}'></div>h[ç›¸ä¼¼åº¦æœç´¢]
            Store --> Search
            Search --> Retrieve[æ£€ç´¢ç›¸å…³ç‰‡æ®µ]
            Retrieve --> Prompt[æ„å»ºå¢å¼ºæç¤º]
            Prompt --> LLM[å¤§æ¨¡å‹ç”Ÿæˆ]
        end
    end
    
    style Docs fill:#ffcccc
    style Query fill:#ccffcc
    style LLM fill:#ccccff
```

### 4.2 å‘é‡æ•°æ®åº“çš„å·¥ä½œæœºåˆ¶

è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªå®Œæ•´çš„RAGç³»ç»Ÿæ¥ç†è§£å…¶å·¥ä½œåŸç†ï¼š

```python
from typing import List, Dict
import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

class SimpleRAGSystem:
    """ç®€åŒ–ç‰ˆRAGç³»ç»Ÿå®ç°"""
    
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedder = SentenceTransformer(model_name)
        
        # åˆå§‹åŒ–ChromaDB
        self.client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory="./chroma_db"
        ))
        
        # åˆ›å»ºæˆ–è·å–é›†åˆ
        self.collection = self.client.get_or_create_collection(
            name="knowledge_base",
            metadata={"hnsw:space": "cosine"}
        )
    
    def add_documents(self, documents: List[str], metadatas: List[Dict] = None):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        # ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = self.embedder.encode(documents).tolist()
        
        # ç”Ÿæˆæ–‡æ¡£ID
        ids = [f"doc_{i}" for i in range(len(documents))]
        
        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        self.collection.add(
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas or [{}] * len(documents),
            ids=ids
        )
        
        print(f"æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“")
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = self.embedder.encode([query]).tolist()
        
        # ç›¸ä¼¼åº¦æœç´¢
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=top_k
        )
        
        return [
            {
                "document": doc,
                "metadata": meta,
                "distance": dist,
                "score": 1 - dist  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
            }
            for doc, meta, dist in zip(
                results['documents'][0],
                results['metadatas'][0],
                results['distances'][0]
            )
        ]
    
    def generate_answer(self, query: str, top_k: int = 3) -> str:
        """ç”Ÿæˆå¢å¼ºå›ç­”"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.search(query, top_k)
        
        if not relevant_docs:
            return "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯"
        
        # æ„å»ºå¢å¼ºæç¤º
        context = "\n\n".join([doc["document"] for doc in relevant_docs])
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š
        
        ä¸Šä¸‹æ–‡ï¼š
        {context}
        
        é—®é¢˜ï¼š{query}
        
        å›ç­”ï¼š"""
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è°ƒç”¨å¤§æ¨¡å‹API
        return f"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œ{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£è¢«ç”¨äºç”Ÿæˆå›ç­”"

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = SimpleRAGSystem()
    
    # æ·»åŠ ç¤ºä¾‹æ–‡æ¡£
    documents = [
        "å¤§æ¨¡å‹è®°å¿†æœºåˆ¶åŒ…æ‹¬å‚æ•°è®°å¿†ã€ä¸Šä¸‹æ–‡è®°å¿†å’Œå¤–éƒ¨è®°å¿†ä¸‰ç§ç±»å‹",
        "RAGæŠ€æœ¯é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆæ¥è§£å†³å¤§æ¨¡å‹çš„çŸ¥è¯†æ›´æ–°é—®é¢˜",
        "å‘é‡æ•°æ®åº“å­˜å‚¨æ–‡æœ¬çš„è¯­ä¹‰åµŒå…¥ï¼Œæ”¯æŒç›¸ä¼¼åº¦æœç´¢",
        "LangChainæä¾›äº†å®Œæ•´çš„RAGå®ç°æ¡†æ¶ï¼ŒåŒ…æ‹¬æ–‡æ¡£åŠ è½½ã€åˆ†å‰²ã€åµŒå…¥ç­‰ç»„ä»¶",
        "ä¸Šä¸‹æ–‡çª—å£é™åˆ¶äº†å¤§æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ï¼ŒRAGå¯ä»¥çªç ´è¿™ä¸ªé™åˆ¶"
    ]
    
    rag.add_documents(documents)
    
    # æµ‹è¯•æŸ¥è¯¢
    query = "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ"
    results = rag.search(query)
    
    print(f"æŸ¥è¯¢: {query}")
    print("æ£€ç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"{i}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
        print(f"   å†…å®¹: {result['document'][:100]}...")
```

### 4.3 æ£€ç´¢ä¸ç”Ÿæˆçš„ååŒä¼˜åŒ–

RAGç³»ç»Ÿçš„æ€§èƒ½å…³é”®åœ¨äº**æ£€ç´¢è´¨é‡**å’Œ**ç”Ÿæˆè´¨é‡**çš„ååŒä¼˜åŒ–ï¼š

```python
class RAGOptimizer:
    """RAGç³»ç»Ÿä¼˜åŒ–å™¨"""
    
    @staticmethod
    def optimize_chunk_size(text: str, chunk_sizes: List[int] = [100, 200, 500, 1000]) -> int:
        """ä¼˜åŒ–æ–‡æœ¬åˆ†å—å¤§å°"""
        
        results = {}
        for chunk_size in chunk_sizes:
            # æ¨¡æ‹Ÿä¸åŒåˆ†å—å¤§å°çš„æ•ˆæœ
            chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
            
            # è®¡ç®—é‡å åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
            overlap_score = len(chunks) * 0.1  # åˆ†å—è¶Šå¤šï¼Œé‡å å¯èƒ½è¶Šå¤š
            
            # è®¡ç®—ä¿¡æ¯å®Œæ•´æ€§
            completeness = min(1.0, len(''.join(chunks)) / len(text))
            
            results[chunk_size] = {
                "chunks": len(chunks),
                "overlap_score": overlap_score,
                "completeness": completeness,
                "score": completeness - overlap_score
            }
        
        # é€‰æ‹©æœ€ä¼˜åˆ†å—å¤§å°
        best_size = max(results.keys(), key=lambda x: results[x]["score"])
        return best_size
    
    @staticmethod
    def improve_retrieval_accuracy(queries: List[str], documents: List[str], top_k: int = 5) -> Dict:
        """æå‡æ£€ç´¢å‡†ç¡®æ€§"""
        
        # å®ç°æŸ¥è¯¢æ‰©å±•
        def expand_query(query: str) -> List[str]:
            """æŸ¥è¯¢æ‰©å±•"""
            expansions = [query]
            
            # æ·»åŠ åŒä¹‰è¯
            synonyms = {
                "å¤§æ¨¡å‹": ["LLM", "å¤§è¯­è¨€æ¨¡å‹", "è¯­è¨€æ¨¡å‹"],
                "è®°å¿†": ["å­˜å‚¨", "ç¼“å­˜", "è®°ä½"],
                "RAG": ["æ£€ç´¢å¢å¼º", "çŸ¥è¯†å¢å¼º"]
            }
            
            for word, syns in synonyms.items():
                if word in query:
                    for syn in syns:
                        expansions.append(query.replace(word, syn))
            
            return expansions
        
        # å®ç°é‡æ’åº
        def rerank_results(query: str, results: List[Dict]) -> List[Dict]:
            """é‡æ’åºæ£€ç´¢ç»“æœ"""
            # åŸºäºæ›´å¤šç‰¹å¾è¿›è¡Œé‡æ’åº
            for result in results:
                # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                query_words = set(query.lower().split())
                doc_words = set(result["document"].lower().split())
                keyword_match = len(query_words & doc_words) / len(query_words)
                
                # æ›´æ–°åˆ†æ•°
                result["reranked_score"] = result["score"] * 0.7 + keyword_match * 0.3
            
            return sorted(results, key=lambda x: x["reranked_score"], reverse=True)
        
        return {
            "query_expansion": expand_query,
            "reranking": rerank_results
        }

# ä¼˜åŒ–ç¤ºä¾‹
optimizer = RAGOptimizer()
best_chunk_size = optimizer.optimize_chunk_size("è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æŠ€æœ¯æ–‡æ¡£..." * 100)
print(f"æœ€ä¼˜åˆ†å—å¤§å°: {best_chunk_size}")
```

## ç¬¬äº”ç« ï¼šé•¿æœŸè®°å¿†-ä»ä¼šè¯åˆ°æŒä¹…åŒ–

### 5.1 å¯¹è¯å†å²çš„å­˜å‚¨ç­–ç•¥

æ„å»ºé•¿æœŸè®°å¿†ç³»ç»Ÿçš„æ ¸å¿ƒåœ¨äº**å¦‚ä½•æœ‰æ•ˆåœ°å­˜å‚¨å’Œæ£€ç´¢å¯¹è¯å†å²**ï¼š

```python
import json
import sqlite3
from datetime import datetime
from typing import List, Dict, Optional
import hashlib

class ConversationMemory:
    """å¯¹è¯é•¿æœŸè®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self, db_path: str = "conversation_memory.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºå¯¹è¯å†å²è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT NOT NULL,
                user_id TEXT NOT NULL,
                message TEXT NOT NULL,
                response TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                topic TEXT,
                keywords TEXT,
                sentiment REAL
            )
        ''')
        
        # åˆ›å»ºç”¨æˆ·ç”»åƒè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS user_profiles (
                user_id TEXT PRIMARY KEY,
                preferences TEXT,
                conversation_summary TEXT,
                last_updated DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def store_conversation(self, session_id: str, user_id: str, 
                          message: str, response: str, 
                          topic: str = None, keywords: List[str] = None):
        """å­˜å‚¨å¯¹è¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æå–å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆTF-IDFï¼‰
        if not keywords:
            keywords = self.extract_keywords(message + " " + response)
        
        cursor.execute('''
            INSERT INTO conversations 
            (session_id, user_id, message, response, topic, keywords)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (session_id, user_id, message, response, topic, json.dumps(keywords)))
        
        conn.commit()
        conn.close()
    
    def extract_keywords(self, text: str, top_k: int = 5) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€åŒ–ç‰ˆå…³é”®è¯æå–
        import re
        from collections import Counter
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶åˆ†è¯
        words = re.findall(r'\b\w+\b', text.lower())
        
        # ç§»é™¤åœç”¨è¯
        stop_words = {'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or', 'but'}
        words = [w for w in words if w not in stop_words and len(w) > 2]
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(words)
        
        # è¿”å›å‰kä¸ªå…³é”®è¯
        return [word for word, _ in word_freq.most_common(top_k)]
    
    def get_conversation_history(self, user_id: str, limit: int = 10) -> List[Dict]:
        """è·å–ç”¨æˆ·å¯¹è¯å†å²"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT message, response, timestamp, topic
            FROM conversations
            WHERE user_id = ?
            ORDER BY timestamp DESC
            LIMIT ?
        ''', (user_id, limit))
        
        results = cursor.fetchall()
        conn.close()
        
        return [
            {
                "message": row[0],
                "response": row[1],
                "timestamp": row[2],
                "topic": row[3]
            }
            for row in results
        ]
    
    def build_user_profile(self, user_id: str) -> Dict:
        """æ„å»ºç”¨æˆ·ç”»åƒ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–ç”¨æˆ·æ‰€æœ‰å¯¹è¯
        cursor.execute('''
            SELECT message, response, topic, keywords
            FROM conversations
            WHERE user_id = ?
        ''', (user_id,))
        
        conversations = cursor.fetchall()
        
        if not conversations:
            return {}
        
        # åˆ†æç”¨æˆ·åå¥½
        topics = [row[2] for row in conversations if row[2]]
        keywords_list = [json.loads(row[3]) for row in conversations if row[3]]
        
        # ç»Ÿè®¡æœ€å¸¸è®¨è®ºçš„è¯é¢˜
        topic_freq = {}
        for topic in topics:
            topic_freq[topic] = topic_freq.get(topic, 0) + 1
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        all_keywords = []
        for keywords in keywords_list:
            all_keywords.extend(keywords)
        
        keyword_freq = {}
        for kw in all_keywords:
            keyword_freq[kw] = keyword_freq.get(kw, 0) + 1
        
        # æ„å»ºç”¨æˆ·ç”»åƒ
        profile = {
            "user_id": user_id,
            "top_topics": sorted(topic_freq.items(), key=lambda x: x[1], reverse=True)[:5],
            "top_keywords": sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:10],
            "total_conversations": len(conversations),
            "conversation_summary": f"ç”¨æˆ·å…±è¿›è¡Œäº†{len(conversations)}æ¬¡å¯¹è¯ï¼Œä¸»è¦å…³æ³¨{topics[0] if topics else 'é€šç”¨è¯é¢˜'}"
        }
        
        # æ›´æ–°ç”¨æˆ·ç”»åƒ
        cursor.execute('''
            INSERT OR REPLACE INTO user_profiles 
            (user_id, preferences, conversation_summary, last_updated)
            VALUES (?, ?, ?, ?)
        ''', (user_id, json.dumps(profile), profile["conversation_summary"], datetime.now()))
        
        conn.commit()
        conn.close()
        
        return profile

# ä½¿ç”¨ç¤ºä¾‹
memory = ConversationMemory()

# æ¨¡æ‹Ÿå¯¹è¯å­˜å‚¨
memory.store_conversation(
    session_id="session_001",
    user_id="user_123",
    message="æˆ‘æƒ³äº†è§£Pythonå¼‚æ­¥ç¼–ç¨‹",
    response="Pythonå¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨async/awaitè¯­æ³•...",
    topic="Pythonå¼‚æ­¥ç¼–ç¨‹"
)

# è·å–ç”¨æˆ·ç”»åƒ
profile = memory.build_user_profile("user_123")
print(json.dumps(profile, indent=2, ensure_ascii=False))
```

### 5.2 å¤šæ¨¡æ€è®°å¿†çš„æ•´åˆ

ç°ä»£AIç³»ç»Ÿéœ€è¦å¤„ç†**æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€**çš„è®°å¿†ï¼š

```python
class MultimodalMemory:
    """å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.memories = {
            "text": {},      # æ–‡æœ¬è®°å¿†
            "image": {},     # å›¾åƒè®°å¿†
            "audio": {},     # éŸ³é¢‘è®°å¿†
            "video": {}      # è§†é¢‘è®°å¿†
        }
    
    def store_multimodal_memory(self, session_id: str, memories: Dict):
        """å­˜å‚¨å¤šæ¨¡æ€è®°å¿†"""
        timestamp = datetime.now().isoformat()
        
        for modality, content in memories.items():
            if modality not in self.memories:
                continue
                
            if modality == "text":
                self.memories[modality][session_id] = {
                    "content": content,
                    "timestamp": timestamp,
                    "type": "text"
                }
            
            elif modality == "image":
                self.memories[modality][session_id] = {
                    "description": content.get("description", ""),
                    "features": content.get("features", []),
                    "timestamp": timestamp,
                    "type": "image"
                }
            
            elif modality == "audio":
                self.memories[modality][session_id] = {
                    "transcript": content.get("transcript", ""),
                    "sentiment": content.get("sentiment", "neutral"),
                    "timestamp": timestamp,
                    "type": "audio"
                }
    
    def retrieve_context(self, session_id: str, query: str = None) -> Dict:
        """æ£€ç´¢å¤šæ¨¡æ€ä¸Šä¸‹æ–‡"""
        context = {}
        
        # æ£€ç´¢ç›¸å…³æ–‡æœ¬è®°å¿†
        if session_id in self.memories["text"]:
            context["text"] = self.memories["text"][session_id]
        
        # åŸºäºæŸ¥è¯¢æ£€ç´¢ç›¸å…³å›¾åƒè®°å¿†
        if query and "image" in self.memories:
            relevant_images = []
            for sid, img_mem in self.memories["image"].items():
                if query.lower() in img_mem.get("description", "").lower():
                    relevant_images.append(img_mem)
            context["images"] = relevant_images
        
        return context

# ä½¿ç”¨ç¤ºä¾‹
multimodal = MultimodalMemory()

# å­˜å‚¨å¤šæ¨¡æ€è®°å¿†
multimodal.store_multimodal_memory("session_001", {
    "text": "ç”¨æˆ·è¯¢é—®å…³äºæœºå™¨å­¦ä¹ ç®—æ³•çš„é€‰æ‹©",
    "image": {
        "description": "ç”¨æˆ·åˆ†äº«çš„ç®—æ³•å¯¹æ¯”å›¾è¡¨",
        "features": ["å†³ç­–æ ‘", "ç¥ç»ç½‘ç»œ", "SVM"]
    },
    "audio": {
        "transcript": "æˆ‘æƒ³äº†è§£ä¸åŒæœºå™¨å­¦ä¹ ç®—æ³•çš„ä¼˜ç¼ºç‚¹",
        "sentiment": "curious"
    }
})
```

## ç¬¬å…­ç« ï¼šå®æˆ˜-æ„å»ºæ™ºèƒ½è®°å¿†ç³»ç»Ÿ

### 6.1 åŸºäºLangChainçš„è®°å¿†å®ç°

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ™ºèƒ½è®°å¿†ç³»ç»Ÿï¼Œé›†æˆLangChainçš„å„ç§è®°å¿†ç»„ä»¶ï¼š

```python
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain.memory.chat_message_histories import SQLChatMessageHistory
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
import os

class IntelligentMemorySystem:
    """æ™ºèƒ½è®°å¿†ç³»ç»Ÿå®Œæ•´å®ç°"""
    
    def __init__(self, openai_api_key: str, db_path: str = "memory_system.db"):
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        # åˆå§‹åŒ–å¤§æ¨¡å‹
        self.llm = ChatOpenAI(temperature=0.7, model="gpt-3.5-turbo")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = OpenAIEmbeddings()
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vectorstore = Chroma(
            embedding_function=self.embeddings,
            persist_directory="./chroma_db"
        )
        
        # åˆå§‹åŒ–å¯¹è¯è®°å¿†
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            chat_memory=SQLChatMessageHistory(
                connection_string=f"sqlite:///{db_path}",
                session_id="default"
            )
        )
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
    
    def add_knowledge(self, documents: List[str], metadatas: List[Dict] = None):
        """æ·»åŠ çŸ¥è¯†åˆ°å‘é‡æ•°æ®åº“"""
        # åˆ†å‰²æ–‡æ¡£
        texts = []
        for doc in documents:
            chunks = self.text_splitter.split_text(doc)
            texts.extend(chunks)
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        self.vectorstore.add_texts(
            texts=texts,
            metadatas=metadatas or [{}] * len(texts)
        )
        
        print(f"æˆåŠŸæ·»åŠ  {len(texts)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")
    
    def create_conversation_chain(self):
        """åˆ›å»ºå¯¹è¯é“¾"""
        return ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )
    
    def chat(self, query: str) -> Dict:
        """æ™ºèƒ½å¯¹è¯"""
        chain = self.create_conversation_chain()
        response = chain({"question": query})
        
        return {
            "answer": response["answer"],
            "source_documents": [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in response.get("source_documents", [])
            ]
        }
    
    def get_conversation_summary(self) -> str:
        """è·å–å¯¹è¯æ‘˜è¦"""
        messages = self.memory.chat_memory.messages
        
        if not messages:
            return "æš‚æ— å¯¹è¯å†å²"
        
        # ç”Ÿæˆå¯¹è¯æ‘˜è¦
        conversation_text = "\n".join([
            f"{msg.type}: {msg.content}" 
            for msg in messages[-10:]  # æœ€è¿‘10æ¡
        ])
        
        summary_prompt = f"è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„æ ¸å¿ƒå†…å®¹ï¼š\n\n{conversation_text}"
        
        summary = self.llm.predict(summary_prompt)
        return summary

# å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
def demonstrate_intelligent_memory():
    """æ¼”ç¤ºæ™ºèƒ½è®°å¿†ç³»ç»Ÿ"""
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    memory_system = IntelligentMemorySystem(
        openai_api_key="your-openai-api-key"
    )
    
    # æ·»åŠ çŸ¥è¯†åº“
    knowledge_docs = [
        """
        LangChainæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚
        å®ƒæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š
        1. æ¨¡å‹I/Oï¼šä¸å¤§æ¨¡å‹äº¤äº’çš„æ ‡å‡†æ¥å£
        2. æ•°æ®è¿æ¥ï¼šåŠ è½½ã€è½¬æ¢ã€å­˜å‚¨å’ŒæŸ¥è¯¢æ•°æ®
        3. é“¾ï¼šç»„åˆç»„ä»¶ä»¥åˆ›å»ºåº”ç”¨ç¨‹åº
        4. è®°å¿†ï¼šåœ¨é“¾ä¹‹é—´æŒä¹…åŒ–åº”ç”¨ç¨‹åºçŠ¶æ€
        5. ä»£ç†ï¼šè®©æ¨¡å‹ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’
        """,
        """
        RAGï¼ˆRetrieval-Augmented Generationï¼‰æŠ€æœ¯é€šè¿‡ä»¥ä¸‹æ­¥éª¤å·¥ä½œï¼š
        1. æ–‡æ¡£åŠ è½½ï¼šä»å„ç§æ¥æºåŠ è½½æ–‡æ¡£
        2. æ–‡æœ¬åˆ†å‰²ï¼šå°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚å½“å¤§å°çš„å—
        3. åµŒå…¥ç”Ÿæˆï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
        4. å‘é‡å­˜å‚¨ï¼šå°†åµŒå…¥å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­
        5. ç›¸ä¼¼åº¦æœç´¢ï¼šæ ¹æ®æŸ¥è¯¢æ‰¾åˆ°ç›¸å…³æ–‡æ¡£
        6. ç­”æ¡ˆç”Ÿæˆï¼šç»“åˆæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”
        """
    ]
    
    memory_system.add_knowledge(knowledge_docs)
    
    # æ¨¡æ‹Ÿå¯¹è¯
    conversations = [
        "ä»€ä¹ˆæ˜¯LangChainï¼Ÿ",
        "RAGæŠ€æœ¯æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
        "LangChainå’ŒRAGæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ",
        "åŸºäºæˆ‘ä»¬ä¹‹å‰çš„è®¨è®ºï¼Œå¦‚ä½•æ„å»ºä¸€ä¸ªæ™ºèƒ½å®¢æœç³»ç»Ÿï¼Ÿ"
    ]
    
    for query in conversations:
        response = memory_system.chat(query)
        print(f"\nğŸ¤” ç”¨æˆ·: {query}")
        print(f"ğŸ¤– AI: {response['answer']}")
        
        if response['source_documents']:
            print(f"ğŸ“š å‚è€ƒäº† {len(response['source_documents'])} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

# è¿è¡Œæ¼”ç¤º
# demonstrate_intelligent_memory()
```

### 6.2 å‘é‡æ•°æ®åº“é€‰å‹ä¸éƒ¨ç½²

è®©æˆ‘ä»¬æ¯”è¾ƒä¸åŒçš„å‘é‡æ•°æ®åº“ï¼Œå¹¶å®ç°ç”Ÿäº§çº§éƒ¨ç½²ï¼š

```python
class VectorDatabaseComparison:
    """å‘é‡æ•°æ®åº“å¯¹æ¯”åˆ†æ"""
    
    def __init__(self):
        self.databases = {
            "Chroma": {
                "type": "æœ¬åœ°/å†…å­˜",
                "scalability": "ä¸­ç­‰",
                "features": ["ç®€å•æ˜“ç”¨", "å†…å­˜å­˜å‚¨", "æŒä¹…åŒ–æ”¯æŒ"],
                "best_for": "åŸå‹å¼€å‘ã€å°å‹åº”ç”¨"
            },
            "FAISS": {
                "type": "æœ¬åœ°/é«˜æ€§èƒ½",
                "scalability": "é«˜",
                "features": ["Facebookå¼€å‘", "GPUåŠ é€Ÿ", "å¤šç§ç´¢å¼•"],
                "best_for": "å¤§è§„æ¨¡ç›¸ä¼¼åº¦æœç´¢"
            },
            "Pinecone": {
                "type": "äº‘æœåŠ¡",
                "scalability": "æé«˜",
                "features": ["æ‰˜ç®¡æœåŠ¡", "è‡ªåŠ¨æ‰©å±•", "å®æ—¶æ›´æ–°"],
                "best_for": "ç”Ÿäº§ç¯å¢ƒã€ä¼ä¸šåº”ç”¨"
            },
            "Weaviate": {
                "type": "å¼€æº/äº‘",
                "scalability": "é«˜",
                "features": ["GraphQL API", "æ··åˆæœç´¢", "å®æ—¶æ›´æ–°"],
                "best_for": "å¤æ‚æŸ¥è¯¢ã€çŸ¥è¯†å›¾è°±"
            },
            "Qdrant": {
                "type": "å¼€æº/äº‘",
                "scalability": "é«˜",
                "features": ["Rustå®ç°", "è¿‡æ»¤æœç´¢", "åˆ†å¸ƒå¼"],
                "best_for": "é«˜æ€§èƒ½ã€å®æ—¶åº”ç”¨"
            }
        }
    
    def get_recommendation(self, use_case: str, scale: str) -> Dict:
        """æ ¹æ®ç”¨ä¾‹æ¨èæ•°æ®åº“"""
        
        recommendations = {
            ("åŸå‹å¼€å‘", "å°"): "Chroma",
            ("ç”Ÿäº§ç¯å¢ƒ", "ä¸­"): "Pinecone",
            ("ç”Ÿäº§ç¯å¢ƒ", "å¤§"): "FAISS + è‡ªå»º",
            ("ä¼ä¸šåº”ç”¨", "å¤§"): "Weaviate",
            ("å®æ—¶åº”ç”¨", "ä¸­"): "Qdrant"
        }
        
        key = (use_case, scale)
        recommended = recommendations.get(key, "Chroma")
        
        return {
            "database": recommended,
            "details": self.databases[recommended],
            "setup_guide": self.get_setup_guide(recommended)
        }
    
    def get_setup_guide(self, db_name: str) -> str:
        """è·å–éƒ¨ç½²æŒ‡å—"""
        
        guides = {
            "Chroma": """
            # Chromaéƒ¨ç½²
            pip install chromadb
            
            # æœ¬åœ°ä½¿ç”¨
            import chromadb
            client = chromadb.PersistentClient(path="./chroma_db")
            """,
            
            "FAISS": """
            # FAISSéƒ¨ç½²
            pip install faiss-cpu  # æˆ– faiss-gpu
            
            # åŸºæœ¬ä½¿ç”¨
            import faiss
            index = faiss.IndexFlatL2(dimension)
            """,
            
            "Pinecone": """
            # Pineconeéƒ¨ç½²
            pip install pinecone-client
            
            # åˆå§‹åŒ–
            import pinecone
            pinecone.init(api_key="your-key")
            """
        }
        
        return guides.get(db_name, "è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£")

# éƒ¨ç½²è„šæœ¬ç¤ºä¾‹
class ProductionDeployment:
    """ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è„šæœ¬"""
    
    @staticmethod
    def create_docker_compose():
        """åˆ›å»ºDocker Composeé…ç½®"""
        
        compose_config = """
version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
  
  app:
    build: .
    ports:
      - "8000:8000"
    depends_on:
      - qdrant
      - redis
    environment:
      - QDRANT_URL=http://qdrant:6333
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./app:/app

volumes:
  redis_data:
"""
        
        with open("docker-compose.yml", "w") as f:
            f.write(compose_config)
        
        return "Docker Composeé…ç½®å·²åˆ›å»º"
    
    @staticmethod
    def create_monitoring_dashboard():
        """åˆ›å»ºç›‘æ§é¢æ¿"""
        
        import streamlit as st
        import plotly.graph_objects as go
        
        st.set_page_config(page_title="è®°å¿†ç³»ç»Ÿç›‘æ§", layout="wide")
        
        st.title("ğŸ§  å¤§æ¨¡å‹è®°å¿†ç³»ç»Ÿç›‘æ§é¢æ¿")
        
        # æ¨¡æ‹Ÿç›‘æ§æ•°æ®
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("æ€»å¯¹è¯æ•°", "1,234", "+12%")
            st.metric("å¹³å‡å“åº”æ—¶é—´", "1.2s", "-5%")
        
        with col2:
            st.metric("çŸ¥è¯†åº“æ–‡æ¡£", "5,678", "+8%")
            st.metric("æ£€ç´¢å‡†ç¡®ç‡", "94.5%", "+2.1%")
        
        with col3:
            st.metric("æ´»è·ƒç”¨æˆ·", "89", "+15%")
            st.metric("ç³»ç»Ÿå¯ç”¨æ€§", "99.9%", "+0.1%")
        
        # æ€§èƒ½è¶‹åŠ¿å›¾
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"],
            y=[1.5, 1.3, 1.2, 1.1, 1.0, 1.2, 1.1],
            mode='lines+markers',
            name='å“åº”æ—¶é—´'
        ))
        
        fig.update_layout(titl<div class="phoenix-chart-container" data-chart='{"type":"mermaid","code":"graph TD\n    A[Transformer] --> B[Mamba]\n    B --> C[RWKV]\n    C --> D[RetNet]\n    D --> E[Hyena]\n    E --> F[æ— é™ä¸Šä¸‹æ–‡]\n    \n    style A fill:#ffcccc\n    style C fill:#ccffcc\n    style F fill:#ccccff"}'></div>"ä¸­")
print(json.dumps(recommendation, indent=2, ensure_ascii=False))
```

## ç¬¬ä¸ƒç« ï¼šæœªæ¥å±•æœ›ä¸æŠ€æœ¯è¶‹åŠ¿

### 7.1 æ— é™ä¸Šä¸‹æ–‡çš„å¯èƒ½æ€§

éšç€æ–°æ¶æ„çš„å‡ºç°ï¼Œå¤§æ¨¡å‹æ­£åœ¨çªç ´ä¼ ç»ŸTransformerçš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼š

```mermaid
graph TD
    A[Transformer] --> B[Mamba]
    B --> C[RWKV]
    C --> D[RetNet]
    D --> E[Hyena]
    E --> F[æ— é™ä¸Šä¸‹æ–‡]
    
    style A fill:#ffcccc
    style C fill:#ccffcc
    style F fill:#ccccff
```

### 7.2 ç¥ç»è®°å¿†ç½‘ç»œçš„æ¼”è¿›

æœªæ¥çš„è®°å¿†ç³»ç»Ÿå°†å…·å¤‡**ç±»è„‘çš„è®°å¿†æœºåˆ¶**ï¼š

```python
class NeuralMemoryNetwork:
    """ç¥ç»è®°å¿†ç½‘ç»œæ¦‚å¿µå®ç°"""
    
    def __init__(self):
        self.memory_systems = {
            "working_memory": self.WorkingMemory(),      # å·¥ä½œè®°å¿†
            "episodic_memory": self.EpisodicMemory(),    # æƒ…æ™¯è®°å¿†
            "semantic_memory": self.SemanticMemory(),    # è¯­ä¹‰è®°å¿†
            "procedural_memory": self.ProceduralMemory() # ç¨‹åºè®°å¿†
        }
    
    class WorkingMemory:
        """å·¥ä½œè®°å¿†ï¼šä¸´æ—¶å­˜å‚¨å’Œå¤„ç†ä¿¡æ¯"""
        def __init__(self, capacity=7):
            self.capacity = capacity
            self.items = []
        
        def add_item(self, item):
            if len(self.items) >= self.capacity:
                self.items.pop(0)
            self.items.append(item)
        
        def get_active_items(self):
            return self.items
    
    class EpisodicMemory:
        """æƒ…æ™¯è®°å¿†ï¼šå­˜å‚¨ä¸ªäººç»å†å’Œäº‹ä»¶"""
        def __init__(self):
            self.episodes = {}
        
        def store_episode(self, event, context, emotion):
            key = f"{event}_{datetime.now().isoformat()}"
            self.episodes[key] = {
                "event": event,
                "context": context,
                "emotion": emotion,
                "timestamp": datetime.now()
            }
    
    class SemanticMemory:
        """è¯­ä¹‰è®°å¿†ï¼šå­˜å‚¨äº‹å®å’Œæ¦‚å¿µ"""
        def __init__(self):
            self.concepts = {}
        
        def store_concept(self, concept, definition, related_concepts):
            self.concepts[concept] = {
                "definition": definition,
                "related": related_concepts,
                "confidence": 0.8
            }
    
    class ProceduralMemory:
        """ç¨‹åºè®°å¿†ï¼šå­˜å‚¨æŠ€èƒ½å’Œç¨‹åº"""
        def __init__(self):
            self.procedures = {}
        
        def store_procedure(self, name, steps, preconditions):
            self.procedures[name] = {
                "steps": steps,
                "preconditions": preconditions,
                "success_rate": 0.0
            }

# æœªæ¥è®°å¿†å¢å¼ºå¤§æ¨¡å‹
class MemoryAugmentedLLM:
    """è®°å¿†å¢å¼ºå¤§æ¨¡å‹"""
    
    def __init__(self, base_model, memory_network):
        self.base_model = base_model
        self.memory = memory_network
        self.learning_rate = 0.001
    
    def process_input(self, input_text, user_id=None):
        """å¤„ç†è¾“å…¥å¹¶æ›´æ–°è®°å¿†"""
        # 1. ä»è®°å¿†ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
        relevant_memories = self.retrieve_memories(input_text, user_id)
        
        # 2. ç»“åˆè®°å¿†ç”Ÿæˆå“åº”
        enhanced_prompt = self.create_enhanced_prompt(input_text, relevant_memories)
        response = self.base_model.generate(enhanced_prompt)
        
        # 3. æ›´æ–°è®°å¿†ç³»ç»Ÿ
        self.update_memories(input_text, response, user_id)
        
        return response
    
    def retrieve_memories(self, query, user_id):
        """æ™ºèƒ½è®°å¿†æ£€ç´¢"""
        memories = []
        
        # ä»ä¸åŒç±»å‹çš„è®°å¿†ä¸­æ£€ç´¢
        for memory_type, memory_system in self.memory.memory_systems.items():
            relevant = memory_system.retrieve(query, user_id)
            memories.extend(relevant)
        
        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶åŠ æƒ
        weighted_memories = self.attention_weighting(query, memories)
        return weighted_memories
    
    def update_memories(self, input_text, response, user_id):
        """åŠ¨æ€æ›´æ–°è®°å¿†"""
        # æå–å…³é”®ä¿¡æ¯
        key_info = self.extract_key_information(input_text, response)
        
        # æ›´æ–°ä¸åŒç±»å‹çš„è®°å¿†
        self.memory.semantic_memory.store_concept(
            concept=key_info["concept"],
            definition=key_info["definition"],
            related_concepts=key_info["related"]
        )
        
        if user_id:
            self.memory.episodic_memory.store_episode(
                event=input_text,
                context=response,
                emotion=self.analyze_sentiment(response)
            )

# æŠ€æœ¯è·¯çº¿å›¾
class TechnologyRoadmap:
    """æŠ€æœ¯å‘å±•è¶‹åŠ¿è·¯çº¿å›¾"""
    
    def __init__(self):
        self.roadmap = {
            "2024": {
                "focus": "RAGä¼˜åŒ–",
                "milestones": [
                    "æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰",
                    "å¤šæ¨¡æ€RAG",
                    "å®æ—¶çŸ¥è¯†æ›´æ–°"
                ]
            },
            "2025": {
                "focus": "è®°å¿†æ¶æ„åˆ›æ–°",
                "milestones": [
                    "ç¥ç»è®°å¿†ç½‘ç»œ",
                    "ä¸ªæ€§åŒ–è®°å¿†ç³»ç»Ÿ",
                    "è·¨ä¼šè¯è®°å¿†"
                ]
            },
            "2026": {
                "focus": "æ— é™ä¸Šä¸‹æ–‡",
                "milestones": [
                    "æ–°æ¶æ„æ™®åŠ",
                    "ç™¾ä¸‡çº§ä¸Šä¸‹æ–‡",
                    "å®æ—¶è®°å¿†å‹ç¼©"
                ]
            },
            "2027": {
                "focus": "é€šç”¨äººå·¥æ™ºèƒ½",
                "milestones": [
                    "ç±»è„‘è®°å¿†ç³»ç»Ÿ",
                    "æŒç»­å­¦ä¹ èƒ½åŠ›",
                    "å¤šæ™ºèƒ½ä½“è®°å¿†å…±äº«"
                ]
            }
        }
    
    def generate_roadmap_chart(self):
        """ç”ŸæˆæŠ€æœ¯è·¯çº¿å›¾"""
        
        timeline = list(self.roadmap.keys())
        milestones = [len(v["milestones"]) for v in self.roadmap.values()]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        # é‡Œç¨‹ç¢‘æ•°é‡è¶‹åŠ¿
        ax1.plot(timeline, milestones, marker='o', linewidth=2, markersize=8)
        ax1.set_title('å¤§æ¨¡å‹è®°å¿†æŠ€æœ¯é‡Œç¨‹ç¢‘æ¼”è¿›', fontsize=14, fontweight='bold')
        ax1.set_ylabel('å¹´åº¦é‡Œç¨‹ç¢‘æ•°é‡', fontsize=12)
        ax1.grid(True, alpha=0.3)
        
        # æŠ€æœ¯ç„¦ç‚¹è¯äº‘
        from wordcloud import WordCloud
        text = " ".join([v["focus"] for v in self.roadmap.values()])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        
        ax2.imshow(wordcloud, interpolation='bilinear')
        ax2.axis('off')
        ax2.set_title('æŠ€æœ¯ç„¦ç‚¹è¯äº‘', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        return fig

# ä½¿ç”¨ç¤ºä¾‹
roadmap = TechnologyRoadmap()
fig = roadmap.generate_roadmap_chart()
plt.savefig('technology_roadmap.png', dpi=300, bbox_inches='tight')
```

## ğŸ¯ æ€»ç»“ä¸è¡ŒåŠ¨æŒ‡å—

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

é€šè¿‡æœ¬æ–‡çš„æ·±å…¥æ¢è®¨ï¼Œæˆ‘ä»¬ç³»ç»Ÿæ€§åœ°ç†è§£äº†å¤§æ¨¡å‹è®°å¿†æœºåˆ¶çš„å®Œæ•´å›¾æ™¯ï¼š

1. **ğŸ§  è®°å¿†å±‚æ¬¡**ï¼šä»å‚æ•°è®°å¿†åˆ°ä¸Šä¸‹æ–‡è®°å¿†ï¼Œå†åˆ°å¤–éƒ¨è®°å¿†ç³»ç»Ÿ
2. **ğŸ” æŠ€æœ¯å®ç°**ï¼šRAGæŠ€æœ¯çš„å®Œæ•´å®ç°æµç¨‹å’Œä¼˜åŒ–ç­–ç•¥
3. **ğŸ’¾ é•¿æœŸè®°å¿†**ï¼šå¯¹è¯å†å²çš„æ™ºèƒ½å­˜å‚¨å’Œç”¨æˆ·ç”»åƒæ„å»º
4. **âš¡ æ€§èƒ½ä¼˜åŒ–**ï¼šå‘é‡æ•°æ®åº“é€‰å‹å’Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
5. **ğŸš€ æœªæ¥è¶‹åŠ¿**ï¼šæ— é™ä¸Šä¸‹æ–‡å’Œç¥ç»è®°å¿†ç½‘ç»œçš„å‘å±•

### ç«‹å³è¡ŒåŠ¨æ¸…å•

**ğŸ“š å­¦ä¹ è·¯å¾„**ï¼š
- [ ] åŠ¨æ‰‹å®ç°æœ¬æ–‡ä¸­çš„RAGç³»ç»Ÿç¤ºä¾‹
- [ ] å°è¯•ä¸åŒçš„å‘é‡æ•°æ®åº“ï¼ˆChromaã€FAISSã€Qdrantï¼‰
- [ ] æ„å»ºä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹
- [ ] å‚ä¸å¼€æºè®°å¿†å¢å¼ºé¡¹ç›®

**ğŸ› ï¸ é¡¹ç›®å®è·µ**ï¼š
- [ ] åŸºäºLangChainæ„å»ºæ™ºèƒ½å®¢æœç³»ç»Ÿ
- [ ] å®ç°ä¸ªæ€§åŒ–å­¦ä¹ åŠ©æ‰‹
- [ ] å¼€å‘ä¼ä¸šçº§çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ
- [ ] åˆ›å»ºå¤šæ¨¡æ€è®°å¿†åº”ç”¨

**ğŸ“Š è¿›é˜¶å­¦ä¹ **ï¼š
- [ ] æ·±å…¥ç ”ç©¶Transformer-XLã€Mambaç­‰æ–°æ¶æ„
- [ ] å­¦ä¹ ç¥ç»å›¾çµæœºå’Œè®°å¿†ç½‘ç»œ
- [ ] æ¢ç´¢è”é‚¦å­¦ä¹ å’Œéšç§ä¿æŠ¤è®°å¿†
- [ ] å…³æ³¨å¤šæ™ºèƒ½ä½“è®°å¿†å…±äº«æŠ€æœ¯

### èµ„æºæ¨è

**ğŸ“– å¿…è¯»è®ºæ–‡**ï¼š
- "Attention Is All You Need" - TransformeråŸå§‹è®ºæ–‡
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" - RAGè®ºæ–‡
- "Mamba: Linear-Time Sequence Modeling with Selective State Spaces" - æ–°æ¶æ„

**ğŸ› ï¸ å¼€æºé¡¹ç›®**ï¼š
- LangChainï¼šå®Œæ•´çš„LLMåº”ç”¨å¼€å‘æ¡†æ¶
- LlamaIndexï¼šä¸“æ³¨äºRAGçš„æ•°æ®æ¡†æ¶
- Chromaï¼šç®€å•æ˜“ç”¨çš„å‘é‡æ•°æ®åº“
- Qdrantï¼šé«˜æ€§èƒ½å‘é‡æœç´¢å¼•æ“

**ğŸ“ åœ¨çº¿è¯¾ç¨‹**ï¼š
- DeepLearning.AIçš„RAGè¯¾ç¨‹
- LangChainå®˜æ–¹æ•™ç¨‹
- Pineconeå‘é‡æœç´¢æŒ‡å—

---

> **ğŸ’¡ æœ€åæ€è€ƒ**ï¼šå¤§æ¨¡å‹è®°å¿†æŠ€æœ¯çš„å‘å±•æ­£åœ¨é‡å¡‘äººæœºäº¤äº’çš„è¾¹ç•Œã€‚ä»ç®€å•çš„é—®ç­”åˆ°å¤æ‚çš„ä¸ªæ€§åŒ–åŠ©æ‰‹ï¼Œè®°å¿†ç³»ç»Ÿå°†æˆä¸ºAIåº”ç”¨çš„æ ¸å¿ƒç«äº‰åŠ›ã€‚æŒæ¡è¿™äº›æŠ€æœ¯ï¼Œä½ å°±èƒ½æ„å»ºçœŸæ­£æ™ºèƒ½çš„ä¸‹ä¸€ä»£AIåº”ç”¨ã€‚
